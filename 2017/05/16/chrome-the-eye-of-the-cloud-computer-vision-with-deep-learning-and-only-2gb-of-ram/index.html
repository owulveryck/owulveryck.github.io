<!doctype html><html lang=en itemscope itemtype=http://schema.org/WebPage><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><title>Chrome, the eye of the cloud - Computer vision with deep learning and only 2Gb of RAM - Unladen swallow - Olivier Wulveryck</title><meta name=renderer content=webkit><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta name=MobileOptimized content=width><meta name=HandheldFriendly content=true><meta name=applicable-device content=pc,mobile><meta name=theme-color content=#f8f5ec><meta name=msapplication-navbutton-color content=#f8f5ec><meta name=apple-mobile-web-app-capable content=yes><meta name=apple-mobile-web-app-status-bar-style content=#f8f5ec><meta name=mobile-web-app-capable content=yes><meta name=author content="Olivier Wulveryck"><meta name=description content="Is this post about Machine Learning? Well, not really, but it is highly related. In this post I will explain how to use a web browser to get information about the environment (pictures and sound). Then, I will present a simple way to process and interact with this information. Why do I do that? At first, simply because I am (trying) to play with tensorflow, chatbots etc, and I need a simple way to grab information to create a training set... But with the evolution of my code, I am now using it alongside with the cloud API of AWS. Welcome to my world."><meta name=keywords content=Go,Dev,IT><meta name=generator content="Hugo 0.58.3"><link rel=canonical href=https://owulveryck.github.io/2017/05/16/chrome-the-eye-of-the-cloud-computer-vision-with-deep-learning-and-only-2gb-of-ram/><link rel=icon href=/favicon.ico><link rel=stylesheet href=/sass/jane.min.af20b78e95c84de86b00a0242a4a77bd2601700e1b250edf27537d957ac0041d.css integrity="sha256-ryC3jpXITehrAKAkKkp3vSYBcA4bJQ7fJ1N9lXrABB0=" media=screen crossorigin=anonymous><meta property=og:title content="Chrome, the eye of the cloud - Computer vision with deep learning and only 2Gb of RAM"><meta property=og:description content="Is this post about Machine Learning? Well, not really, but it is highly related. In this post I will explain how to use a web browser to get information about the environment (pictures and sound). Then, I will present a simple way to process and interact with this information. Why do I do that? At first, simply because I am (trying) to play with tensorflow, chatbots etc, and I need a simple way to grab information to create a training set... But with the evolution of my code, I am now using it alongside with the cloud API of AWS. Welcome to my world."><meta property=og:type content=article><meta property=og:url content=https://owulveryck.github.io/2017/05/16/chrome-the-eye-of-the-cloud-computer-vision-with-deep-learning-and-only-2gb-of-ram/><meta property=og:image content="https://lh3.googleusercontent.com/nYhPnY2I-e9rpqnid9u9aAODz4C04OycEGxqHG5vxFnA35OGmLMrrUmhM9eaHKJ7liB-=w300"><meta property=article:published_time content=2017-05-16T21:43:46+02:00><meta property=article:modified_time content=2017-05-16T21:43:46+02:00><meta itemprop=name content="Chrome, the eye of the cloud - Computer vision with deep learning and only 2Gb of RAM"><meta itemprop=description content="Is this post about Machine Learning? Well, not really, but it is highly related. In this post I will explain how to use a web browser to get information about the environment (pictures and sound). Then, I will present a simple way to process and interact with this information. Why do I do that? At first, simply because I am (trying) to play with tensorflow, chatbots etc, and I need a simple way to grab information to create a training set... But with the evolution of my code, I am now using it alongside with the cloud API of AWS. Welcome to my world."><meta itemprop=datePublished content=2017-05-16T21:43:46&#43;02:00><meta itemprop=dateModified content=2017-05-16T21:43:46&#43;02:00><meta itemprop=wordCount content=2402><meta itemprop=image content="https://lh3.googleusercontent.com/nYhPnY2I-e9rpqnid9u9aAODz4C04OycEGxqHG5vxFnA35OGmLMrrUmhM9eaHKJ7liB-=w300"><meta itemprop=keywords content=ML,Chrome,><meta name=twitter:card content=summary_large_image><meta name=twitter:image content="https://lh3.googleusercontent.com/nYhPnY2I-e9rpqnid9u9aAODz4C04OycEGxqHG5vxFnA35OGmLMrrUmhM9eaHKJ7liB-=w300"><meta name=twitter:title content="Chrome, the eye of the cloud - Computer vision with deep learning and only 2Gb of RAM"><meta name=twitter:description content="Is this post about Machine Learning? Well, not really, but it is highly related. In this post I will explain how to use a web browser to get information about the environment (pictures and sound). Then, I will present a simple way to process and interact with this information. Why do I do that? At first, simply because I am (trying) to play with tensorflow, chatbots etc, and I need a simple way to grab information to create a training set... But with the evolution of my code, I am now using it alongside with the cloud API of AWS. Welcome to my world."><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script><script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]--><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-69673850-1','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script></head><body><div id=mobile-navbar class=mobile-navbar><div class=mobile-header-logo><a href=/ class=logo>owulveryck's blog</a></div><div class=mobile-navbar-icon><span></span><span></span><span></span></div></div><nav id=mobile-menu class="mobile-menu slideout-menu"><ul class=mobile-menu-list><li class=mobile-menu-item><a class=menu-item-link href=https://owulveryck.github.io/>This is Home</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://owulveryck.github.io/post/>Archives</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://owulveryck.github.io/tags/>Tags</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://owulveryck.github.io/categories/>Categories</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://owulveryck.github.io/about/>About</a></li></ul></nav><link rel=stylesheet href=/lib/photoswipe/photoswipe.min.css><link rel=stylesheet href=/lib/photoswipe/default-skin/default-skin.min.css><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><header id=header class="header container"><div class=logo-wrapper><a href=/ class=logo>owulveryck's blog</a></div><nav class=site-navbar><ul id=menu class=menu><li class=menu-item><a class=menu-item-link href=https://owulveryck.github.io/>This is Home</a></li><li class=menu-item><a class=menu-item-link href=https://owulveryck.github.io/post/>Archives</a></li><li class=menu-item><a class=menu-item-link href=https://owulveryck.github.io/tags/>Tags</a></li><li class=menu-item><a class=menu-item-link href=https://owulveryck.github.io/categories/>Categories</a></li><li class=menu-item><a class=menu-item-link href=https://owulveryck.github.io/about/>About</a></li></ul></nav></header><div id=mobile-panel><main id=main class="main bg-llight"><div class=content-wrapper><div id=content class="content container"><article class="post bg-white"><header class=post-header><h1 class=post-title>Chrome, the eye of the cloud - Computer vision with deep learning and only 2Gb of RAM</h1><div class=post-meta><time datetime=2017-05-16 class=post-time>2017-05-16</time></div></header><div class=post-toc id=post-toc><h2 class=post-toc-title>Table of Contents</h2><div class=post-toc-content><nav id=TableOfContents><ul><li><ul><li><a href=#so-what-makes-tensorflow-so-great>So what makes tensorflow so great?</a><ul><li><a href=#bindings>Bindings</a></li><li><a href=#ml-and-neuron-network-examples>ML and neuron network examples</a></li><li><a href=#built-in-computation-at-scale>Built-in computation at scale</a></li><li><a href=#gcp-s-ml-engine>GCP&rsquo;s ML engine</a></li></ul></li></ul></li><li><a href=#so-what>So What?</a></li><li><a href=#chrome-as-the-eye-of-the-computer>Chrome as the eye of the computer</a><ul><li><a href=#getusermedia>getUserMedia</a></li><li><a href=#websockets>Websockets</a><ul><li><a href=#connecting-to-the-websocket>Connecting to the websocket</a></li><li><a href=#messages>Messages</a></li><li><a href=#sending-pictures-to-the-websocket-actually-seeing>Sending pictures to the websocket: actually seeing</a></li></ul></li><li><a href=#bonus-ear-and-voice>Bonus: ear and voice</a></li></ul></li><li><a href=#the-brain-cortical>The <em>brain</em>: <strong>Cortical</strong></a><ul><li><a href=#cortexes><em>Cortexes</em></a><ul><li><a href=#a-tensorflow-cortex-runnig-locally>A tensorflow cortex runnig locally</a><ul><li><a href=#demo>Demo</a></li></ul></li><li><a href=#in-the-cloud-with-aws>In the cloud with AWS</a></li></ul></li></ul></li><li><a href=#any-real-application>Any real application?</a></li></ul></nav></div></div><div class=post-content><p><strong>TL;DR:</strong> Thank you for passing by. This article is, as usual, geek oriented. However, if you are not a geek, and/or you are in a hurry, you can jump to the conclusion: <em><a href=#any-real-application>Any real application?</a></em></p><p>During the month of may, I have had the chance to attend to the Google Next event in London and the dotAI in Paris. In both conferences I learned a lot about machine learning.</p><p>What those great speakers have taught me is that you should not reinvent the wheel in AI. Actually a lot of research is done and there are very good implementation of the latest efficient algorithm.</p><p><em>The tool</em> that every engineer that wants to try AI must know is <a href=https://www.tensorflow.org/>tensorflow</a>. Tensorflow is a generic framework that has been developed by Google&rsquo;s Machine Intelligence research organization. The tool has been open-sourced last year and has reached the v1.0 earlier this year.</p><h2 id=so-what-makes-tensorflow-so-great>So what makes tensorflow so great?</h2><h3 id=bindings>Bindings</h3><p>First of all, it has bindings so it can be used within various programming languages such as:</p><ul><li>python</li><li>c++</li><li>java</li><li>go</li></ul><p>However, to be honest, mainly python and c++ are described in the documentation. And to be even more honest I think that python is the language that you should use to prototype applications.</p><h3 id=ml-and-neuron-network-examples>ML and neuron network examples</h3><p>Tensorflow is easy to use for machine learning, and a lot of deep-learning implementation are available.
Actually it is very easy to download a trained model and use it to recognize some pictures for example.</p><h3 id=built-in-computation-at-scale>Built-in computation at scale</h3><p>Tensorflow&rsquo;s model has a built-in way to perform distributed computation. It is really important as machine learning is usually a very intensive task in term of computation.</p><h3 id=gcp-s-ml-engine>GCP&rsquo;s ML engine</h3><p>Tensorflow is the engine used by Google for their service called ML engine.
That means that you can write your function locally and run them serverless on the cloud.
You only pay for what you have effectively consumed.
That means for example that you can train a neuron network on GCP (so you don&rsquo;t need GPU. TPU, or whatever computing power) and transfer your model locally.</p><p>For example, this is how the mobile app &ldquo;google translate&rdquo; works. A pre-trained model is downloaded on your phone, and the live translation is done locally.</p><p><img src=http://technews.wpengine.netdna-cdn.com/wp-content/uploads/2015/01/www.lanacion.com_.ar_.jpg alt=Image></p><p><em>Note</em> Other ML services from GCP such as cloud vision, translate, or image search, are &ldquo;just&rdquo; API that query a neuron network with a model trained by google.</p><h1 id=so-what>So What?</h1><p>I want to play with image recognition. Actually I already did a test with AWS&rsquo;s rekognition service (<a href=/2016/12/16/image-rekognition-with-a-webcam-go-and-aws..html>See this post</a>). However, the problems were:</p><ul><li>I relied on a low-level webcam implementation. Therefore, the code was not portable;</li><li>I had no preview of what my computer was looking at;</li><li>I could not execute it on any mobile app for a demo;</li></ul><p>As I am using a Chromebook for a while, I found a solution: Using a Javascript API and the Chrome browser to access the camera. Then, the pictures can be transfered to a backend via a websocket. The backend would do the ML and reply with whatever information via the websocket. I can then display the result or even use the voice api of Chrome to tell the result loud.</p><h1 id=chrome-as-the-eye-of-the-computer>Chrome as the eye of the computer</h1><p>The idea is to get a video stream and grab pictures from this stream in order to activate a neural network.</p><p>I will present different objects in front of my webcam, and their name will be displayed on the screen.</p><p>The architecture is client server: The Chrome is the eye of my bot, it communicates with the brain (a webservice in go that is running a pre-trained tensorflow neural network) via a websocket.</p><p><strong>The rest of this paragraph is geek/javascript, if you&rsquo;re not interested you can jump to the next paragraph about the brain implementation called <em><a href=#the-brain-cortical>Cortical</a></em></strong></p><h2 id=getusermedia>getUserMedia</h2><p>I am using the Web API <a href=https://developer.mozilla.org/en-US/docs/Web/API/MediaDevices/getUserMedia>MediaDevices.getUserMedia()</a> to open the webcam and get the stream.</p><p>This API is compatible with chrome on desktop <em>and</em> mobile on Android phone (but not on iOS). This means that I will be able to use a mobile phone as an &ldquo;eye&rdquo; of my bot.</p><p>See the <a href=https://developer.mozilla.org/en-US/docs/Web/API/MediaDevices/getUserMedia#Browser_compatibility>compatibility matrix here</a></p><p>Here is the code to get access to the camera and display the video stream:</p><p><em>html</em><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code class=language-html data-lang=html><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-html data-lang=html><span class=p>&lt;</span><span class=nt>body</span><span class=p>&gt;</span>
  <span class=p>&lt;</span><span class=nt>video</span> <span class=na>autoplay</span> <span class=na>id</span><span class=o>=</span><span class=s>&#34;webcam&#34;</span><span class=p>&gt;&lt;/</span><span class=nt>video</span><span class=p>&gt;</span>
<span class=p>&lt;/</span><span class=nt>body</span><span class=p>&gt;</span></code></pre></td></tr></table></div></div></p><p><em>Javascript</em><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code class=language-js data-lang=js><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-js data-lang=js><span class=c1>// use MediaDevices API
</span><span class=c1>// docs: https://developer.mozilla.org/en-US/docs/Web/API/MediaDevices/getUserMedia
</span><span class=c1></span><span class=k>if</span> <span class=p>(</span><span class=nx>navigator</span><span class=p>.</span><span class=nx>mediaDevices</span><span class=p>)</span> <span class=p>{</span>
    <span class=c1>// access the web cam
</span><span class=c1></span>    <span class=nx>navigator</span><span class=p>.</span><span class=nx>mediaDevices</span><span class=p>.</span><span class=nx>getUserMedia</span><span class=p>({</span><span class=nx>video</span><span class=o>:</span> <span class=kc>true</span><span class=p>})</span>
      <span class=c1>// permission granted:
</span><span class=c1></span>      <span class=p>.</span><span class=nx>then</span><span class=p>(</span><span class=kd>function</span><span class=p>(</span><span class=nx>stream</span><span class=p>)</span> <span class=p>{</span>
          <span class=nx>video</span><span class=p>.</span><span class=nx>src</span> <span class=o>=</span> <span class=nb>window</span><span class=p>.</span><span class=nx>URL</span><span class=p>.</span><span class=nx>createObjectURL</span><span class=p>(</span><span class=nx>stream</span><span class=p>);</span>
      <span class=p>})</span>
      <span class=c1>// permission denied:
</span><span class=c1></span>      <span class=p>.</span><span class=k>catch</span><span class=p>(</span><span class=kd>function</span><span class=p>(</span><span class=nx>error</span><span class=p>)</span> <span class=p>{</span>
          <span class=nb>document</span><span class=p>.</span><span class=nx>body</span><span class=p>.</span><span class=nx>textContent</span> <span class=o>=</span> <span class=s1>&#39;Could not access the camera. Error: &#39;</span> <span class=o>+</span> <span class=nx>error</span><span class=p>.</span><span class=nx>name</span><span class=p>;</span>
      <span class=p>});</span>
<span class=p>}</span>
</code></pre></td></tr></table></div></div></p><h2 id=websockets>Websockets</h2><p>According to Wikipedia&rsquo;s definition, Websocket is <em>a computer communications protocol, providing full-duplex communication channels over a single TCP connection</em>.
The full duplex mode is important in my architecture.</p><p>Let me explain why with a simple use case:</p><p>Imagine that your eye captures a scene and sends it to the brain for analysis. In a classic RESTfull architecture, the browser (the eye) would perform a POST request.
The brain would reply with a process ID, and the eye would poll the endpoint every x seconds to get the processing status.</p><p>This can be tedious in case of multiple stimuli.</p><p>Thanks to the websocket, the server can send the query, and the server will send an event back once the processing is done.
Of course this is stateless in a sort, as the query is lost once the browser is closed.</p><p>Another use case would be to get a stimulus from another &ldquo;sense&rdquo;. For example, imagine that you want to &ldquo;warn&rdquo; the end user that he has been mentioned in a tweet. The brain can be in charge of polling
twitter, and it would send a message through the websocket in case of event.</p><h3 id=connecting-to-the-websocket>Connecting to the websocket</h3><p>A websocket URI is prefixed by <code>ws</code> or <code>wss</code> if the communication is encrypted (aka https).
This code allows a connection through ws(s).</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code class=language-js data-lang=js><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-js data-lang=js><span class=kd>var</span> <span class=nx>ws</span>
<span class=c1>// Connecting the websocket
</span><span class=c1></span><span class=kd>var</span> <span class=nx>loc</span> <span class=o>=</span> <span class=nb>window</span><span class=p>.</span><span class=nx>location</span><span class=p>,</span> <span class=nx>new_uri</span><span class=p>;</span>
<span class=k>if</span> <span class=p>(</span><span class=nx>loc</span><span class=p>.</span><span class=nx>protocol</span> <span class=o>===</span> <span class=s2>&#34;https:&#34;</span><span class=p>)</span> <span class=p>{</span>
  <span class=nx>new_uri</span> <span class=o>=</span> <span class=s2>&#34;wss:&#34;</span><span class=p>;</span>
<span class=p>}</span> <span class=k>else</span> <span class=p>{</span>
  <span class=nx>new_uri</span> <span class=o>=</span> <span class=s2>&#34;ws:&#34;</span><span class=p>;</span>
<span class=p>}</span>
<span class=nx>new_uri</span> <span class=o>+=</span> <span class=s2>&#34;//&#34;</span> <span class=o>+</span> <span class=nx>loc</span><span class=p>.</span><span class=nx>host</span> <span class=o>+</span> <span class=s2>&#34;/ws&#34;</span><span class=p>;</span>
<span class=nx>ws</span> <span class=o>=</span> <span class=k>new</span> <span class=nx>WebSocket</span><span class=p>(</span><span class=nx>new_uri</span><span class=p>);</span>
</code></pre></td></tr></table></div></div><h3 id=messages>Messages</h3><p>Web socket communication is message oriented. A message can be sent simply by calling the function <code>ws.send(message)</code>. Websockets are supporting texts and binary messages.
But for this test only text messages will be used (images will be encoded in base64).</p><p>The browser implementation of a websocket in javascript is event based.
When the server sends a message, an interruption is fired and the <code>ws.onmessage</code> call is triggered.</p><p>This code will display the message received on the console:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code class=language-js data-lang=js><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-js data-lang=js><span class=nx>ws</span><span class=p>.</span><span class=nx>onmessage</span> <span class=o>=</span> <span class=kd>function</span><span class=p>(</span><span class=nx>event</span><span class=p>)</span> <span class=p>{</span>
  <span class=nx>console</span><span class=p>.</span><span class=nx>log</span><span class=p>(</span><span class=s2>&#34;Received:&#34;</span> <span class=o>+</span> <span class=nx>event</span><span class=p>.</span><span class=nx>data</span><span class=p>);</span>
<span class=p>};</span>
</code></pre></td></tr></table></div></div><h3 id=sending-pictures-to-the-websocket-actually-seeing>Sending pictures to the websocket: actually seeing</h3><p>I didn&rsquo;t find a way to send the video stream to the brain via the websocket. Therefore, I will do what everybody does: create a canvas and &ldquo;take&rdquo; a picture from the video:</p><p>The method <a href=https://developer.mozilla.org/en-US/docs/Web/API/HTMLCanvasElement/toDataURL>toDataURL()</a> will take care of encoding the picture in a well-known format (png or jpeg).</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code class=language-js data-lang=js><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-js data-lang=js><span class=kd>function</span> <span class=nx>takeSnapshot</span><span class=p>()</span> <span class=p>{</span>
  <span class=kd>var</span> <span class=nx>context</span><span class=p>;</span>
  <span class=kd>var</span> <span class=nx>width</span> <span class=o>=</span> <span class=nx>video</span><span class=p>.</span><span class=nx>offsetWidth</span>
  <span class=p>,</span> <span class=nx>height</span> <span class=o>=</span> <span class=nx>video</span><span class=p>.</span><span class=nx>offsetHeight</span><span class=p>;</span>

  <span class=nx>canvas</span> <span class=o>=</span> <span class=nx>canvas</span> <span class=o>||</span> <span class=nb>document</span><span class=p>.</span><span class=nx>createElement</span><span class=p>(</span><span class=s1>&#39;canvas&#39;</span><span class=p>);</span>
  <span class=nx>canvas</span><span class=p>.</span><span class=nx>width</span> <span class=o>=</span> <span class=nx>width</span><span class=p>;</span>
  <span class=nx>canvas</span><span class=p>.</span><span class=nx>height</span> <span class=o>=</span> <span class=nx>height</span><span class=p>;</span>

  <span class=nx>context</span> <span class=o>=</span> <span class=nx>canvas</span><span class=p>.</span><span class=nx>getContext</span><span class=p>(</span><span class=s1>&#39;2d&#39;</span><span class=p>);</span>
  <span class=nx>context</span><span class=p>.</span><span class=nx>drawImage</span><span class=p>(</span><span class=nx>video</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=nx>width</span><span class=p>,</span> <span class=nx>height</span><span class=p>);</span>

  <span class=kd>var</span> <span class=nx>dataURI</span> <span class=o>=</span> <span class=nx>canvas</span><span class=p>.</span><span class=nx>toDataURL</span><span class=p>(</span><span class=s1>&#39;image/jpeg&#39;</span><span class=p>)</span>
  <span class=c1>//...
</span><span class=c1></span><span class=p>};</span>
</code></pre></td></tr></table></div></div><p>To make the processing in the brain easier, I will serialize the video into a json object and sending it via the websocket:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code class=language-js data-lang=js><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-js data-lang=js><span class=kd>var</span> <span class=nx>message</span> <span class=o>=</span> <span class=p>{</span><span class=s2>&#34;dataURI&#34;</span><span class=o>:</span><span class=p>{}};</span>
<span class=nx>message</span><span class=p>.</span><span class=nx>dataURI</span><span class=p>.</span><span class=nx>content</span> <span class=o>=</span> <span class=nx>dataURI</span><span class=p>.</span><span class=nx>split</span><span class=p>(</span><span class=s1>&#39;,&#39;</span><span class=p>)[</span><span class=mi>1</span><span class=p>];</span>
<span class=nx>message</span><span class=p>.</span><span class=nx>dataURI</span><span class=p>.</span><span class=nx>contentType</span> <span class=o>=</span> <span class=nx>dataURI</span><span class=p>.</span><span class=nx>split</span><span class=p>(</span><span class=s1>&#39;,&#39;</span><span class=p>)[</span><span class=mi>0</span><span class=p>].</span><span class=nx>split</span><span class=p>(</span><span class=s1>&#39;:&#39;</span><span class=p>)[</span><span class=mi>1</span><span class=p>].</span><span class=nx>split</span><span class=p>(</span><span class=s1>&#39;;&#39;</span><span class=p>)[</span><span class=mi>0</span><span class=p>]</span>
<span class=kd>var</span> <span class=nx>json</span> <span class=o>=</span> <span class=nx>JSON</span><span class=p>.</span><span class=nx>stringify</span><span class=p>(</span><span class=nx>message</span><span class=p>);</span>
<span class=nx>ws</span><span class=p>.</span><span class=nx>send</span><span class=p>(</span><span class=nx>json</span><span class=p>);</span>
</code></pre></td></tr></table></div></div><h2 id=bonus-ear-and-voice>Bonus: ear and voice</h2><p>It is relatively easy to make chrome speak out loud the message received. This snippet will speak out loud the message Received:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code class=language-js data-lang=js><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-js data-lang=js><span class=kd>function</span> <span class=nx>talk</span><span class=p>(</span><span class=nx>message</span><span class=p>)</span> <span class=p>{</span>
  <span class=kd>var</span> <span class=nx>utterance</span> <span class=o>=</span> <span class=k>new</span> <span class=nx>SpeechSynthesisUtterance</span><span class=p>(</span><span class=nx>message</span><span class=p>);</span>
  <span class=nb>window</span><span class=p>.</span><span class=nx>speechSynthesis</span><span class=p>.</span><span class=nx>speak</span><span class=p>(</span><span class=nx>utterance</span><span class=p>);</span>
<span class=p>}</span>
</code></pre></td></tr></table></div></div><p>Therefore, simply adding a call to this function in the &ldquo;onmessage&rdquo; event of the websocket will trigger the voice of Chrome.</p><p>Listening is a bit trickier. It is done by a call to the <code>webkitSpeechRecognition();</code> method. This <a href=https://developers.google.com/web/updates/2013/01/Voice-Driven-Web-Apps-Introduction-to-the-Web-Speech-API>blog post</a> explains in detail how this works.</p><p>The call is also event based. What&rsquo;s important is that, in chrome, by default, it will use an API call to the Google&rsquo;s engine. Therefore the recognition won&rsquo;t work offline.</p><p>When the language processing is done by chrome, five potential sentences are stored in a json array.
The following snippet will take the most relevant one and send it to the brain via the websocket:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code class=language-js data-lang=js><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-js data-lang=js><span class=nx>recognition</span><span class=p>.</span><span class=nx>onresult</span> <span class=o>=</span> <span class=kd>function</span><span class=p>(</span><span class=nx>event</span><span class=p>)</span> <span class=p>{</span> 
  <span class=k>for</span> <span class=p>(</span><span class=kd>var</span> <span class=nx>i</span> <span class=o>=</span> <span class=nx>event</span><span class=p>.</span><span class=nx>resultIndex</span><span class=p>;</span> <span class=nx>i</span> <span class=o>&lt;</span> <span class=nx>event</span><span class=p>.</span><span class=nx>results</span><span class=p>.</span><span class=nx>length</span><span class=p>;</span> <span class=o>++</span><span class=nx>i</span><span class=p>)</span> <span class=p>{</span>
    <span class=k>if</span> <span class=p>(</span><span class=nx>event</span><span class=p>.</span><span class=nx>results</span><span class=p>[</span><span class=nx>i</span><span class=p>].</span><span class=nx>isFinal</span><span class=p>)</span> <span class=p>{</span>
      <span class=nx>final_transcript</span> <span class=o>+=</span> <span class=nx>event</span><span class=p>.</span><span class=nx>results</span><span class=p>[</span><span class=nx>i</span><span class=p>][</span><span class=mi>0</span><span class=p>].</span><span class=nx>transcript</span><span class=p>;</span>
      <span class=nx>ws</span><span class=p>.</span><span class=nx>send</span><span class=p>(</span><span class=nx>final_transcript</span><span class=p>);</span>
    <span class=p>}</span>
  <span class=p>}</span>
<span class=p>};</span>
</code></pre></td></tr></table></div></div><p><em>Now that we have set up the senses, let&rsquo;s make a &ldquo;brain&rdquo;</em></p><h1 id=the-brain-cortical>The <em>brain</em>: <strong>Cortical</strong></h1><p><img src=https://github.com/owulveryck/cortical/raw/master/doc/cortical.png alt=Picture></p><p>Now, let me explain what is, according to me, the <strong>most interesting part</strong> of this post. By now, all that I have done is a bit of javascript to grab a picture. This is not a big deal, and there is no machine learning yet (besides the speech recognition built-in in chrome).
What I need now is to actually process the messages so the computer can tell what it sees.</p><p>For this purpose I have developed a message dispatcher. This dispatcher, called <em>Cortical</em> is available on <a href=https://github.com/owulveryck/cortical>github</a></p><p>Here is an extract from the README of the project:</p><hr><p><strong>What is Cortical?</strong></p><p>Cortical is a go <del>framework</del> <del>middleware</del> piece of code that acts as a message dispatcher. The messages are transmitted in full duplex over a websocket.
Cortical is therefore a very convenient way to distribute messages to &ldquo;processing units&rdquo; (other go functions) and to get the responses back in a <strong>concurrent</strong> and <strong>asynchronous</strong> way.</p><p>The &ldquo;processing units&rdquo; are called <em>Cortexes</em> and do not need to be aware of any web mechanism.</p><hr><p>So far, so good, I can simply create a handler to receive the messages sent by the chrome browser in go:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code class=language-go data-lang=go><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-go data-lang=go><span class=nx>brain</span> <span class=o>:=</span> <span class=o>&amp;</span><span class=nx>cortical</span><span class=p>.</span><span class=nx>Cortical</span><span class=p>{</span>
    <span class=nx>Upgrader</span><span class=p>:</span> <span class=nx>websocket</span><span class=p>.</span><span class=nx>Upgrader</span><span class=p>{},</span>
    <span class=nx>Cortexes</span><span class=p>:</span>  <span class=p>[]</span><span class=nx>cortical</span><span class=p>.</span><span class=nx>Cortex</span><span class=p>{</span>
                    <span class=o>&amp;</span><span class=nx>sampleTensorflowCortex</span><span class=p>{},</span> <span class=c1>// cortex?
</span><span class=c1></span>               <span class=p>},</span> 
<span class=p>}</span>
<span class=nx>http</span><span class=p>.</span><span class=nf>HandleFunc</span><span class=p>(</span><span class=s>&#34;/ws&#34;</span><span class=p>,</span> <span class=nx>brain</span><span class=p>.</span><span class=nx>ServeWS</span><span class=p>)</span>
<span class=nx>log</span><span class=p>.</span><span class=nf>Fatal</span><span class=p>(</span><span class=nx>http</span><span class=p>.</span><span class=nf>ListenAndServe</span><span class=p>(</span><span class=s>&#34;:8080&#34;</span><span class=p>,</span> <span class=kc>nil</span><span class=p>))</span></code></pre></td></tr></table></div></div><p><em>Note</em>: <strong>Concurrency</strong> and <strong>asynchronicity</strong> are really built in <em>Cortical</em>, this is what makes this code so helpful actually.</p><h2 id=cortexes><em>Cortexes</em></h2><p>Cortexes are processing units. That is the place where messages are analyzed and where the ML magic happens.</p><p>From the readme, I quote:</p><hr><p>A cortex is any go code that provides two functions:</p><ul><li>A &ldquo;send&rdquo; function that returns a channel of <code>[]byte</code>. The content of the channel is sent to the websocket once available (cf <a href=https://godoc.org/github.com/owulveryck/cortical#GetInfoFromCortexFunc><code>GetInfoFromCortexFunc</code></a>)</li><li>A &ldquo;receive&rdquo; method that take a pointer of <code>[]byte</code>. This function is called each time a message is received (cf <a href=https://godoc.org/github.com/owulveryck/cortical#SendInfoToCortex><code>SendInfoToCortex</code></a>)</li></ul><p>A cortex object must therefore be compatible with the <code>cortical.Cortex</code> interface:</p><hr><p>Ok, let&rsquo;s build Cortexes!</p><h3 id=a-tensorflow-cortex-runnig-locally>A tensorflow cortex runnig locally</h3><p>The tensorflow go package is a binding to the <code>libtensorflow.so</code>. It has a very nice example described in the <a href=https://godoc.org/github.com/tensorflow/tensorflow/tensorflow/go#ex-package>godoc here</a>.
This example is using a pre-trained inception model (<a href=http://arxiv.org/abs/1512.00567>http://arxiv.org/abs/1512.00567</a>).
The program starts by downloading the pre-trained model, creates a graph, and try to guess labels on a given image.</p><p>I will simply add the expected interface to transform this example into a Cortex compatible with my previous declaration (<em>some error check and some code has been omited for clarity</em>):</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code class=language-go data-lang=go><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-go data-lang=go><span class=kd>type</span> <span class=nx>sampleTensorflowCortex</span> <span class=kd>struct</span><span class=p>{}</span>

<span class=kd>func</span> <span class=p>(</span><span class=nx>t</span> <span class=o>*</span><span class=nx>sampleTensorflowCortex</span><span class=p>)</span> <span class=nf>NewCortex</span><span class=p>(</span><span class=nx>ctx</span> <span class=nx>context</span><span class=p>.</span><span class=nx>Context</span><span class=p>)</span> <span class=p>(</span><span class=nx>cortical</span><span class=p>.</span><span class=nx>GetInfoFromCortexFunc</span><span class=p>,</span> <span class=nx>cortical</span><span class=p>.</span><span class=nx>SendInfoToCortex</span><span class=p>)</span> <span class=p>{</span>
        <span class=nx>c</span> <span class=o>:=</span> <span class=nb>make</span><span class=p>(</span><span class=kd>chan</span> <span class=p>[]</span><span class=kt>byte</span><span class=p>)</span>
        <span class=nx>class</span> <span class=o>:=</span> <span class=o>&amp;</span><span class=nx>classifier</span><span class=p>{</span>
                <span class=nx>c</span><span class=p>:</span> <span class=nx>c</span><span class=p>,</span>
        <span class=p>}</span>
        <span class=k>return</span> <span class=nx>class</span><span class=p>.</span><span class=nx>Send</span><span class=p>,</span> <span class=nx>class</span><span class=p>.</span><span class=nx>Receive</span>
<span class=p>}</span>

<span class=kd>type</span> <span class=nx>classifier</span> <span class=kd>struct</span> <span class=p>{</span>
        <span class=nx>c</span> <span class=kd>chan</span> <span class=p>[]</span><span class=kt>byte</span>
<span class=p>}</span>

<span class=kd>func</span> <span class=p>(</span><span class=nx>t</span> <span class=o>*</span><span class=nx>classifier</span><span class=p>)</span> <span class=nf>Receive</span><span class=p>(</span><span class=nx>ctx</span> <span class=nx>context</span><span class=p>.</span><span class=nx>Context</span><span class=p>,</span> <span class=nx>b</span> <span class=o>*</span><span class=p>[]</span><span class=kt>byte</span><span class=p>)</span> <span class=p>{</span>
        <span class=kd>var</span> <span class=nx>m</span> <span class=nx>message</span>
        <span class=c1>// omited for brievety 
</span><span class=c1></span>        <span class=nx>tensor</span><span class=p>,</span> <span class=nx>err</span> <span class=o>:=</span> <span class=nf>makeTensorFromImage</span><span class=p>(</span><span class=nx>m</span><span class=p>.</span><span class=nx>DataURI</span><span class=p>.</span><span class=nx>Content</span><span class=p>)</span>
        <span class=nx>output</span><span class=p>,</span> <span class=nx>err</span> <span class=o>:=</span> <span class=nx>session</span><span class=p>.</span><span class=nf>Run</span><span class=p>(</span>
                <span class=kd>map</span><span class=p>[</span><span class=nx>tf</span><span class=p>.</span><span class=nx>Output</span><span class=p>]</span><span class=o>*</span><span class=nx>tf</span><span class=p>.</span><span class=nx>Tensor</span><span class=p>{</span>
                        <span class=nx>graph</span><span class=p>.</span><span class=nf>Operation</span><span class=p>(</span><span class=s>&#34;input&#34;</span><span class=p>).</span><span class=nf>Output</span><span class=p>(</span><span class=mi>0</span><span class=p>):</span> <span class=nx>tensor</span><span class=p>,</span>
                <span class=p>},</span>
                <span class=p>[]</span><span class=nx>tf</span><span class=p>.</span><span class=nx>Output</span><span class=p>{</span> <span class=nx>graph</span><span class=p>.</span><span class=nf>Operation</span><span class=p>(</span><span class=s>&#34;output&#34;</span><span class=p>).</span><span class=nf>Output</span><span class=p>(</span><span class=mi>0</span><span class=p>),</span>
                <span class=p>},</span> <span class=kc>nil</span><span class=p>)</span>
        <span class=nx>probabilities</span> <span class=o>:=</span> <span class=nx>output</span><span class=p>[</span><span class=mi>0</span><span class=p>].</span><span class=nf>Value</span><span class=p>().([][]</span><span class=kt>float32</span><span class=p>)[</span><span class=mi>0</span><span class=p>]</span>
        <span class=nx>label</span> <span class=o>:=</span> <span class=nf>printBestLabel</span><span class=p>(</span><span class=nx>probabilities</span><span class=p>,</span> <span class=nx>labelsfile</span><span class=p>)</span>
        <span class=nx>t</span><span class=p>.</span><span class=nx>c</span> <span class=o>&lt;-</span> <span class=p>[]</span><span class=nb>byte</span><span class=p>(</span><span class=nx>fmt</span><span class=p>.</span><span class=nf>Sprintf</span><span class=p>(</span><span class=s>&#34;%v (%2.0f%%)&#34;</span><span class=p>,</span> <span class=nx>label</span><span class=p>.</span><span class=nx>Label</span><span class=p>,</span> <span class=nx>label</span><span class=p>.</span><span class=nx>Probability</span><span class=o>*</span><span class=mf>100.0</span><span class=p>))</span>
<span class=p>}</span>

<span class=kd>func</span> <span class=p>(</span><span class=nx>t</span> <span class=o>*</span><span class=nx>classifier</span><span class=p>)</span> <span class=nf>Send</span><span class=p>(</span><span class=nx>ctx</span> <span class=nx>context</span><span class=p>.</span><span class=nx>Context</span><span class=p>)</span> <span class=kd>chan</span> <span class=p>[]</span><span class=kt>byte</span> <span class=p>{</span>
      <span class=k>return</span> <span class=nx>t</span><span class=p>.</span><span class=nx>c</span>
<span class=p>}</span></code></pre></td></tr></table></div></div><h4 id=demo>Demo</h4><p>This demo has been made with my Chromebook that has only 2 Gb or RAM. The tensorflow library is compiled without any optimization.
It works!</p><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/psb9r_YhwiY style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div><p>The code is <a href=https://github.com/owulveryck/socketcam>here</a>.</p><h3 id=in-the-cloud-with-aws>In the cloud with AWS</h3><p>Now that I have seen that it works on my Chromebook, I can maybe use the cloud API to recognize some faces for example.
Let&rsquo;s try with AWS&rsquo; rekognition service.</p><p>I will use the face compare API to check whether the person in front of the webcam is me.
I will provide a sample picture of me to the cortex.</p><p>I took the sample picture at work, to make the task a little bit trickier for the engine because the environment will not match what it will see.</p><p>I won&rsquo;t dig into the code that can be found <a href=https://github.com/owulveryck/socketcam/blob/master/processors/rekognition/main.go>here</a>.</p><p>And does it work?</p><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/KbvRr7XXoyE style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div><p>Cool!</p><h1 id=any-real-application>Any real application?</h1><p>This is really fun and exciting.Now I will be able to code a memory cortex to fetch a training set. Then I will play with tensorflow. And do not think that everything has already been done, this area is full of surprises to come (<a href=https://en.wikipedia.org/wiki/Moravec%27s_paradox>This is the Moravec&rsquo;s paradox</a>).</p><p>However, on top of that, we can imagine a lot of application. Actually, this service is working out-of-the box on Android (and it will on iOS as soon as Apple supports the getUSerMedia interface).
I imagine a simple web app (no need for an APK), that would warn you when it sees someone he knows.</p><p>I also imagine a web gallery, and the webcam would watch your reaction in front of different items and then tells you which one has been your favorite.</p><p>Indeed, there may be a lot of great application for e-commerce.</p><p>You can turn your laptop into a CCTV system so it can warn you when an unknown person is in the room. We would do a preprocessing to detect humans before actually sending the info to the cloud. That would be cheaper and a lot more efficient than the crappy CV implemented in the webcam.</p><p>And finally, combined with react.js, this can be used to do magic keynotes&hellip; But I will keep that for another story.</p><p>As a conclusion, I will put this XKCD of September 2014. It is only three years old, and yet, so many things have already changed:</p><p><img src=https://imgs.xkcd.com/comics/tasks.png alt=XKCD></p></div><div class=post-copyright><p class=copyright-item><span class=item-title>Author</span>
<span class=item-content>Olivier Wulveryck</span></p><p class=copyright-item><span class=item-title>LastMod</span>
<span class=item-content>2017-05-16</span></p><p class=copyright-item><span class=item-title>License</span>
<span class=item-content><a rel="license noopener" href=https://creativecommons.org/licenses/by-nc-nd/4.0/ target=_blank>CC BY-NC-ND 4.0</a></span></p></div><footer class=post-footer><div class=post-tags><a href=https://owulveryck.github.io/tags/ml/>ML</a>
<a href=https://owulveryck.github.io/tags/chrome/>Chrome</a></div><nav class=post-nav><a class=prev href=/2017/06/01/analyzing-a-parodic-trailer-nsfw-with-google-cloud-video-intelligence/><i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="18" height="18"><path d="M691.908486 949.511495l75.369571-89.491197c10.963703-12.998035 10.285251-32.864502-1.499144-44.378743L479.499795 515.267417l277.93508-310.326815c11.338233-12.190647 11.035334-32.285311-.638543-44.850487l-80.46666-86.564541c-11.680017-12.583596-30.356378-12.893658-41.662889-.716314L257.233596 494.235404c-11.332093 12.183484-11.041474 32.266891.657986 44.844348l80.46666 86.564541c1.772366 1.910513 3.706415 3.533476 5.750981 4.877077l306.620399 321.703933C662.505829 963.726242 680.945807 962.528973 691.908486 949.511495z"/></svg></i><span class="prev-text nav-default">Analyzing a parodic trailer (NSFW) with Google Cloud Video Intelligence</span>
<span class="prev-text nav-mobile">Prev</span></a>
<a class=next href=/2017/04/14/i-have-tried-extreme-programming-within-a-sprint-and-i-think-it-is-an-excellent-agile-method-for-the-ops/><span class="next-text nav-default">I have tried Extreme Programming within a sprint and I think it is an excellent agile method for the Ops!</span>
<span class="prev-text nav-mobile">Next</span>
<i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="18" height="18"><path d="M332.091514 74.487481l-75.369571 89.491197c-10.963703 12.998035-10.285251 32.864502 1.499144 44.378743l286.278095 300.375162L266.565125 819.058374c-11.338233 12.190647-11.035334 32.285311.638543 44.850487l80.46666 86.564541c11.680017 12.583596 30.356378 12.893658 41.662889.716314l377.434212-421.426145c11.332093-12.183484 11.041474-32.266891-.657986-44.844348l-80.46666-86.564541c-1.772366-1.910513-3.706415-3.533476-5.750981-4.877077L373.270379 71.774697C361.493148 60.273758 343.054193 61.470003 332.091514 74.487481z"/></svg></i></a></nav></footer></article><div class=disqus-comment><div class=disqus-button id=load_disqus onclick=load_disqus()>Show Disqus Comments</div><div id=disqus_thread></div><script type=text/javascript>var disqus_config=function(){this.page.url="https://owulveryck.github.io/2017/05/16/chrome-the-eye-of-the-cloud-computer-vision-with-deep-learning-and-only-2gb-of-ram/";};function load_disqus(){if(window.location.hostname==='localhost')return;var dsq=document.createElement('script');dsq.type='text/javascript';dsq.async=true;var disqus_shortname='owulveryck';dsq.src='//'+disqus_shortname+'.disqus.com/embed.js';(document.getElementsByTagName('head')[0]||document.getElementsByTagName('body')[0]).appendChild(dsq);$('#load_disqus').remove();};</script><noscript>Please enable JavaScript to view the
<a href=http://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript></div></div></div></main><footer id=footer class=footer><div class=icon-links><a href=https://twitter.com/owulveryck rel="me noopener" class=iconfont title=twitter target=_blank><svg class="icon" viewBox="0 0 1264 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="36" height="36"><path d="M1229.8616 18.043658c0 0-117.852626 63.135335-164.151872 67.344358-105.225559-164.151872-505.082682-92.598492-437.738325 223.078185C278.622548 312.675223 89.216542 47.506814 89.216542 47.506814s-117.852626 189.406006 75.762402 345.139833C127.097743 396.85567 55.544363 371.601535 55.544363 371.601535S26.081207 535.753407 253.368414 615.724832c-21.045112 29.463156-113.643603 8.418045-113.643603 8.418045s25.254134 143.10676 231.496229 180.987961c-143.10676 130.479693-387.230056 92.598492-370.393967 105.225559 206.242095 189.406006 1119.599946 231.496229 1128.01799-643.98042C1179.353331 249.539887 1263.533778 123.269217 1263.533778 123.269217s-130.479693 37.881201-138.897738 33.672179C1225.652577 98.015083 1229.8616 18.043658 1229.8616 18.043658"/></svg></a><a href=https://www.linkedin.com/in/olivierwulveryck/ rel="me noopener" class=iconfont title=linkedin target=_blank><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="33" height="33"><path d="M872.405333 872.618667H720.768v-237.610667c0-56.661333-1.152-129.578667-79.018667-129.578667-79.061333.0-91.136 61.653333-91.136 125.397334v241.792H398.976V384H544.64v66.602667h1.962667c20.352-38.4 69.845333-78.933333 143.786666-78.933334 153.642667.0 182.058667 101.12 182.058667 232.746667v268.202667zM227.712 317.141333a87.978667 87.978667.0 0 1-88.021333-88.106666A88.064 88.064.0 1 1 227.712 317.141333zm76.032 555.477334H151.68V384h152.064v488.618667zM948.266667.0h-872.704C33.792.0.0 33.024.0 73.770667v876.458666C0 991.018667 33.792 1024 75.562667 1024h872.576C989.866667 1024 1024 991.018667 1024 950.229333V73.770667C1024 33.024 989.866667.0 948.138667.0h.128z"/></svg></a><a href=http://github.com/owulveryck rel="me noopener" class=iconfont title=github target=_blank><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="36" height="36"><path d="M512 12.672c-282.88.0-512 229.248-512 512 0 226.261333 146.688 418.133333 350.08 485.76 25.6 4.821333 34.986667-11.008 34.986667-24.618667.0-12.16-.426667-44.373333-.64-87.04-142.421333 30.890667-172.458667-68.693333-172.458667-68.693333C188.672 770.986667 155.008 755.2 155.008 755.2c-46.378667-31.744 3.584-31.104 3.584-31.104 51.413333 3.584 78.421333 52.736 78.421333 52.736 45.653333 78.293333 119.850667 55.68 149.12 42.581333 4.608-33.109333 17.792-55.68 32.426667-68.48-113.706667-12.8-233.216-56.832-233.216-253.013333.0-55.893333 19.84-101.546667 52.693333-137.386667-5.76-12.928-23.04-64.981333 4.48-135.509333.0.0 42.88-13.738667 140.8 52.48 40.96-11.392 84.48-17.024 128-17.28 43.52.256 87.04 5.888 128 17.28 97.28-66.218667 140.16-52.48 140.16-52.48 27.52 70.528 10.24 122.581333 5.12 135.509333 32.64 35.84 52.48 81.493333 52.48 137.386667.0 196.693333-119.68 240-233.6 252.586667 17.92 15.36 34.56 46.762667 34.56 94.72.0 68.522667-.64 123.562667-.64 140.202666.0 13.44 8.96 29.44 35.2 24.32C877.44 942.592 1024 750.592 1024 524.672c0-282.752-229.248-512-512-512"/></svg></a><a href=https://owulveryck.github.io/index.xml rel="noopener alternate" type=application/rss&#43;xml class=iconfont title=rss target=_blank><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="30" height="30"><path d="M819.157333 1024C819.157333 574.592 449.408 204.8.0 204.8V0c561.706667.0 1024 462.293333 1024 1024H819.157333zM140.416 743.04a140.8 140.8.0 0 1 140.501333 140.586667A140.928 140.928.0 0 1 140.074667 1024C62.72 1024 0 961.109333.0 883.626667S62.933333 743.082667 140.416 743.04zM678.784 1024h-199.04c0-263.210667-216.533333-479.786667-479.744-479.786667V345.173333c372.352.0 678.784 306.517333 678.784 678.826667z"/></svg></a></div><div class=copyright><span class=power-by>Powered by <a class=hexo-link href=https://gohugo.io>Hugo</a></span>
<span class=division>|</span>
<span class=theme-info>Theme - <a class=theme-link href=https://github.com/xianmin/hugo-theme-jane>Jane</a></span>
<span class=copyright-year>&copy;
2015 -
2020
<span class=heart><i class=iconfont><svg class="icon" viewBox="0 0 1025 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="14" height="14"><path d="M1000.1 247.9c-15.5-37.3-37.6-70.6-65.7-98.9-54.4-54.8-125.8-85-201-85-85.7.0-166 39-221.4 107.4C456.6 103 376.3 64 290.6 64c-75.1.0-146.5 30.4-201.1 85.6-28.2 28.5-50.4 61.9-65.8 99.3-16 38.8-24 79.9-23.6 122.2.7 91.7 40.1 177.2 108.1 234.8 3.1 2.6 6 5.1 8.9 7.8 14.9 13.4 58 52.8 112.6 102.7 93.5 85.5 209.9 191.9 257.5 234.2 7 6.1 15.8 9.5 24.9 9.5 9.2.0 18.1-3.4 24.9-9.5 34.5-30.7 105.8-95.9 181.4-165 74.2-67.8 150.9-138 195.8-178.2 69.5-57.9 109.6-144.4 109.9-237.3.1-42.5-8-83.6-24-122.2z" fill="#8a8a8a"/></svg></i></span><span class=author>Olivier Wulveryck</span></span></div></footer><div class=back-to-top id=back-to-top><i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="35" height="35"><path d="M510.866688 227.694839 95.449397 629.218702h235.761562L329.15309 958.01517h362.40389L691.55698 628.188232l241.942331-3.089361L510.866688 227.694839zM63.840492 63.962777h894.052392v131.813095H63.840492V63.962777v0zm0 0"/></svg></i></div></div><script type=text/javascript src=/lib/jquery/jquery-3.2.1.min.js></script><script type=text/javascript src=/lib/slideout/slideout-1.0.1.min.js></script><script type=text/javascript src=/js/main.638251f4230630f0335d8c6748e53a96f94b72670920b60c09a56fdc8bece214.js integrity="sha256-Y4JR9CMGMPAzXYxnSOU6lvlLcmcJILYMCaVv3Ivs4hQ=" crossorigin=anonymous></script><script type=text/javascript src=/js/load-photoswipe.js></script><script type=text/javascript src=/lib/photoswipe/photoswipe.min.js></script><script type=text/javascript src=/lib/photoswipe/photoswipe-ui-default.min.js></script></body></html>