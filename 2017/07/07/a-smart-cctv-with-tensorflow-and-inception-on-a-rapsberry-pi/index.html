<!doctype html><html lang=en itemscope itemtype=http://schema.org/WebPage><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><title>A &#34;Smart&#34; CCTV with Tensorflow, and Inception? On a rapsberry pi? - Unladen swallow - Olivier Wulveryck</title><meta name=renderer content=webkit><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta name=MobileOptimized content=width><meta name=HandheldFriendly content=true><meta name=applicable-device content=pc,mobile><meta name=theme-color content=#f8f5ec><meta name=msapplication-navbutton-color content=#f8f5ec><meta name=apple-mobile-web-app-capable content=yes><meta name=apple-mobile-web-app-status-bar-style content=#f8f5ec><meta name=mobile-web-app-capable content=yes><meta name=author content="Olivier Wulveryck"><meta name=description content="Imagine a CCTV at home that would trigger an alert when it detects a movement. Ok, this is easy. Imagine a CCTV that would trigger an alert when it detects a human. A little bit trickier. Now imagine a CCTV that would trigger an alert when it sees someone who is not from the family."><meta name=keywords content=Go,Dev,IT><meta name=generator content="Hugo 0.58.3"><link rel=canonical href=https://owulveryck.github.io/2017/07/07/a-smart-cctv-with-tensorflow-and-inception-on-a-rapsberry-pi/><link rel=icon href=/favicon.ico><link rel=stylesheet href=/sass/jane.min.af20b78e95c84de86b00a0242a4a77bd2601700e1b250edf27537d957ac0041d.css integrity="sha256-ryC3jpXITehrAKAkKkp3vSYBcA4bJQ7fJ1N9lXrABB0=" media=screen crossorigin=anonymous><meta property=og:title content='A "Smart" CCTV with Tensorflow, and Inception? On a rapsberry pi?'><meta property=og:description content="Imagine a CCTV at home that would trigger an alert when it detects a movement. Ok, this is easy. Imagine a CCTV that would trigger an alert when it detects a human. A little bit trickier. Now imagine a CCTV that would trigger an alert when it sees someone who is not from the family."><meta property=og:type content=article><meta property=og:url content=https://owulveryck.github.io/2017/07/07/a-smart-cctv-with-tensorflow-and-inception-on-a-rapsberry-pi/><meta property=og:image content=https://owulveryck.github.io/assets/images/tensorflowserving-4.png><meta property=article:published_time content=2017-07-07T21:06:46+02:00><meta property=article:modified_time content=2017-07-07T21:06:46+02:00><meta itemprop=name content='A "Smart" CCTV with Tensorflow, and Inception? On a rapsberry pi?'><meta itemprop=description content="Imagine a CCTV at home that would trigger an alert when it detects a movement. Ok, this is easy. Imagine a CCTV that would trigger an alert when it detects a human. A little bit trickier. Now imagine a CCTV that would trigger an alert when it sees someone who is not from the family."><meta itemprop=datePublished content=2017-07-07T21:06:46&#43;02:00><meta itemprop=dateModified content=2017-07-07T21:06:46&#43;02:00><meta itemprop=wordCount content=2365><meta itemprop=image content=https://owulveryck.github.io/assets/images/tensorflowserving-4.png><meta itemprop=keywords content><meta name=twitter:card content=summary_large_image><meta name=twitter:image content=https://owulveryck.github.io/assets/images/tensorflowserving-4.png><meta name=twitter:title content='A "Smart" CCTV with Tensorflow, and Inception? On a rapsberry pi?'><meta name=twitter:description content="Imagine a CCTV at home that would trigger an alert when it detects a movement. Ok, this is easy. Imagine a CCTV that would trigger an alert when it detects a human. A little bit trickier. Now imagine a CCTV that would trigger an alert when it sees someone who is not from the family."><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script><script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]--><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-69673850-1','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script></head><body><div id=mobile-navbar class=mobile-navbar><div class=mobile-header-logo><a href=/ class=logo>owulveryck's blog</a></div><div class=mobile-navbar-icon><span></span><span></span><span></span></div></div><nav id=mobile-menu class="mobile-menu slideout-menu"><ul class=mobile-menu-list><li class=mobile-menu-item><a class=menu-item-link href=https://owulveryck.github.io/>This is Home</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://owulveryck.github.io/post/>Archives</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://owulveryck.github.io/tags/>Tags</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://owulveryck.github.io/categories/>Categories</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://owulveryck.github.io/about/>About</a></li></ul></nav><link rel=stylesheet href=/lib/photoswipe/photoswipe.min.css><link rel=stylesheet href=/lib/photoswipe/default-skin/default-skin.min.css><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><header id=header class="header container"><div class=logo-wrapper><a href=/ class=logo>owulveryck's blog</a></div><nav class=site-navbar><ul id=menu class=menu><li class=menu-item><a class=menu-item-link href=https://owulveryck.github.io/>This is Home</a></li><li class=menu-item><a class=menu-item-link href=https://owulveryck.github.io/post/>Archives</a></li><li class=menu-item><a class=menu-item-link href=https://owulveryck.github.io/tags/>Tags</a></li><li class=menu-item><a class=menu-item-link href=https://owulveryck.github.io/categories/>Categories</a></li><li class=menu-item><a class=menu-item-link href=https://owulveryck.github.io/about/>About</a></li></ul></nav></header><div id=mobile-panel><main id=main class="main bg-llight"><div class=content-wrapper><div id=content class="content container"><article class="post bg-white"><header class=post-header><h1 class=post-title>A &#34;Smart&#34; CCTV with Tensorflow, and Inception? On a rapsberry pi?</h1><div class=post-meta><time datetime=2017-07-07 class=post-time>2017-07-07</time></div></header><div class=post-toc id=post-toc><h2 class=post-toc-title>Table of Contents</h2><div class=post-toc-content><nav id=TableOfContents><ul><li><a href=#machine-learning-and-computer-vision>Machine learning and computer vision</a><ul><li><a href=#tools>Tools</a><ul><li><a href=#tensorflow>Tensorflow</a></li><li><a href=#inception>Inception</a></li></ul></li></ul></li><li><a href=#geek>Geek</a><ul><li><a href=#phase-1-recognizing-usual-people>Phase 1: recognizing usual people</a><ul><li><a href=#getting-a-training-set-full-of-people>Getting a training set full of people</a></li></ul></li><li><a href=#learning-phase>Learning phase</a></li><li><a href=#using-the-model-with-go>Using the model with go</a><ul><li><a href=#getting-the-pictures>Getting the pictures</a></li><li><a href=#normalizing-the-picture>Normalizing the picture</a></li></ul></li></ul></li><li><a href=#conclusion>Conclusion</a><ul><li><a href=#running-it-on-a-laptop>Running it on a laptop</a></li><li><a href=#further-work>Further work</a><ul><li><a href=#running-on-arm>Running on ARM</a><ul><li><a href=#performances>Performances</a></li></ul></li><li><a href=#detecting-only-the-family>Detecting only the family</a></li></ul></li></ul></li></ul></nav></div></div><div class=post-content><p>Imagine a CCTV at home that would trigger an alert when it detects a movement.</p><p>Ok, this is easy.</p><p>Imagine a CCTV that would trigger an alert when it detects a human (and not the cat).</p><p>A little bit trickier.</p><p>Now imagine a CCTV that would trigger an alert when it sees someone who is not from the family&hellip;</p><p><strong>Disclaimer</strong>: This article will not cover everything. I may post a second article later (or not). As you may now, I am doing those experiments during the night as all of this is not part of my job. I hope that I will find time to actually conclude the experiment. If you are a geek and you want to test that with me, feel free and welcome to contact me via the comments or via twitter <a href=https://twitter.com/owulveryck>@owulveryck</a>.
In this article, I will describe the method. I will also retrain a neural network to detect people. I will also use a GO static binary to run it live and evaluate the performances. By the end, I will try a static cross compilation to run it on a raspberry pi, but as my rpi is by now out-of-order, I will test it on qemu.</p><h1 id=machine-learning-and-computer-vision>Machine learning and computer vision</h1><p>Machine learning and tooling around it has increased and gained in efficiency in the past years. it now &ldquo;easy&rdquo; to code a model that can be trained to detect and classify elements from a picture.</p><p>Cloud providers are offering services that can instantly tag and label elements from an image. To achieve the goal of the CCTV, it would be really easy to use, for example, <a href=https://aws.amazon.com/rekognition/>AWS rekognition</a>, train the model, and post a request for each image seen.</p><p>This solution presents a couple of problems:</p><ul><li><p>The network bandwidth: you need a reliable network and a bandwidth big enough to upload the flow of images</p></li><li><p>The cost: these services are cheap for thousand images, but consider about 1 fps to process (I don&rsquo;t even dream of 24fps), it is 86400 images a day and 2.6 million images a month&hellip; and considering that 1 million images are 1000 dollar&hellip;</p></li></ul><p>I don&rsquo;t even talk about network latency because my CCTV would be pseudo-real-time and the ms of latency can be neglected.</p><p>The best solution would be to run the computer vision locally. There are several methods to detect people. The most up-to-date-and-accurate one is based on machine learning and precisely on neural network.</p><p><strong>The very simplified principle of neural network and computer vision:</strong></p><p>There is a lot of literacy on the web around that, but here a very small explanation to understand the rest of this post:</p><p>Imagine a picture as a sequence of numbers from 0 to 1 (0 for black 1 for white for example). Imagine a mathematical equation <code>f</code>.
You do not know what the content of <code>f</code> is. You only tell the computer: &ldquo;guess a value of <code>f</code> such as <code>f(pictures of a man) = man</code>.
Then you feed him with a lot of pictures of men, and for every picture, the computer not only guess a function <code>f</code> but it adapts it so it can detect every man in every picture.</p><p>Sounds magical?</p><p>Actually, the computer does not start with a void <code>f</code> function. You provide it with a kind of skeleton that you call the neural network.
A neural network is a network of tiny function (neuron) that are applied on the decomposed values of the input (such as a pixel in a photo).</p><p>Depending on the mathematical function coded in the neuron, it is activated by its inputs (there can be several inputs for a single neuron) or not.</p><p>You can use several layers of neurons. Each layer is composed of neurons feed by the outputs of the neurons of the previous layer.</p><p>The pictures used to feed the model is called the training set.
You also use a test set (same kind of pictures), that is used to check whether your model generalized well and actually converge to your goal.</p><p>I won&rsquo;t dig any further into this description. You can read papers about the <a href=https://en.wikipedia.org/wiki/Perceptron>perceptron</a> for more accuracy in the description. I heavily recommend this article if you know a little bit of go: <a href=https://appliedgo.net/perceptron/>Perceptrons - the most basic form of a neural network</a></p><p><center><link rel=stylesheet href=/css/hugo-easy-gallery.css><div class=box><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><div class=img><img itemprop=thumbnail src=https://imgs.xkcd.com/comics/machine_learning.png alt="XKCD 1838"></div><a href=https://xkcd.com/1838/ itemprop=contentUrl></a><figcaption><p>XKCD 1838</p></figcaption></figure></div></center></p><h2 id=tools>Tools</h2><h3 id=tensorflow>Tensorflow</h3><p>I have already blogged about tensorflow. Tensorflow is not a ML library. It is a mathematical library. It is self-described as <em>an open source software library for numerical computation using data flow graphs. Nodes in the graph represent mathematical operations, while the graph edges represent the multidimensional data arrays (tensors) communicated between them.</em></p><p>It is, therefore, an excellent tool, suitable for machine learning and especially for dealing with neural networks. It is brilliant with computer vision as the pictures are arrays of pixels and if you add the colour, you can represent every picture by a tensor.</p><p>Even better, the models generated by tensorflow can be saved once learned and transferred to another device. For example, you can train your models on very powerful machines and simply copy the resulted graph to your client (for example a phone). The graph can then be applied to an input taken from the device such as a photo.</p><h3 id=inception>Inception</h3><p>&ldquo;<a href=https://research.google.com/pubs/pub43022.html>Inception</a>&rdquo; is a deep convolutional neural network architecture used to classify images originally developed by Google.</p><p>Inception is exceptionally accurate for computer vision. It can reach 78% accuracy in &ldquo;Top-1&rdquo; and 93.9% in &ldquo;Top-5&rdquo;. That means that if you feed the model with a picture of sunglasses, you have 93.9% chance that the algorithm detects sunglasses amongst the top 5 results.</p><p>On top of that, Inception is implemented with Tensorflow, and well documented. Therefore, it easy &ldquo;easy&rdquo; to use it, to train it and &ldquo;to retrain it&rdquo;.</p><p>here is a graphical representation of the inception v3 model. You can see the different layers of the model as explained earlier.</p><p><center><div class=box><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><div class=img><img itemprop=thumbnail src=https://raw.githubusercontent.com/tensorflow/models/master/inception/g3doc/inception_v3_architecture.png alt="Inception v3 architecture"></div><a href=https://github.com/tensorflow/models/tree/master/inception itemprop=contentUrl></a><figcaption><p>Inception v3 architecture</p></figcaption></figure></div></center></p><p>Actually, training the model is a very long process (several days on very efficient machines with GPU). But some folks (at google?) have discovered that retraining only the last layer of the neural network for new classes of pictures was giving good results.</p><h1 id=geek>Geek</h1><p>I am using the excellent blog post <a href=https://www.tensorflow.org/tutorials/image_retraining>How to Retrain Inception&rsquo;s Final Layer for New Categories</a>.
The purpose of the article is to retrain the network in order to give it the ability to categorize (recognize) different kind of flowers.
I will use exactly the same principle to recognize a class &ldquo;people&rdquo;.</p><p>I will perform the task on a spot instance on AWS (to get it cheap), and download the model to use it locally from a go code.</p><h2 id=phase-1-recognizing-usual-people>Phase 1: recognizing usual people</h2><p>To keep it simple, I&rsquo;ve created a &ldquo;class&rdquo; people with the flowers classes. It means that I simply added a directory &ldquo;people&rdquo; to my &ldquo;flowers&rdquo; for now.</p><pre><code>[~/flower_photos]$ ls -lrt
total 696
-rw-r----- 1 ubuntu ubuntu 418049 Feb  9  2016 LICENSE.txt
drwx------ 2 ubuntu ubuntu  45056 Feb 10  2016 tulips
drwx------ 2 ubuntu ubuntu  36864 Feb 10  2016 sunflowers
drwx------ 2 ubuntu ubuntu  36864 Feb 10  2016 roses
drwx------ 2 ubuntu ubuntu  57344 Feb 10  2016 dandelion
drwx------ 2 ubuntu ubuntu  36864 Feb 10  2016 daisy
drwxrwxr-x 2 ubuntu ubuntu  77824 Jul  7 14:26 people
</code></pre><h3 id=getting-a-training-set-full-of-people>Getting a training set full of people</h3><p>I need a training set of people. That means that I need a certain amount of pictures actually representing some people.
Nowadays it is easy to get training set for free (as in free speech) on the web.</p><p><em>Note</em> You can see that, by offering services, the GAFA is increasing its training set to make their service more powerful than ever.
<center>&lt; tweet 857609299731791872 &gt;</center></p><p>Let&rsquo;s get back to my experiment:
I download pictures of people from <a href="http://www.image-net.org/api/text/imagenet.synset.geturls?wnid=n07942152">http://www.image-net.org/api/text/imagenet.synset.geturls?wnid=n07942152</a></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>curl -s  <span class=s2>&#34;http://www.image-net.org/api/text/imagenet.synset.geturls?wnid=n07942152&#34;</span> <span class=p>|</span> <span class=se>\
</span><span class=se></span>sed <span class=s1>&#39;s/^M//&#39;</span> <span class=p>|</span> <span class=se>\
</span><span class=se></span><span class=k>while</span> <span class=nb>read</span> file
<span class=k>do</span>
  curl -m <span class=m>3</span> -O <span class=nv>$file</span>
<span class=k>done</span></code></pre></td></tr></table></div></div><p>Then I remove all &ldquo;non-image&rdquo; files:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell><span class=k>for</span> i in <span class=k>$(</span>ls *jpg<span class=k>)</span>
<span class=k>do</span>
    file <span class=nv>$i</span> <span class=p>|</span> egrep -qi <span class=s2>&#34;jpeg|png&#34;</span> <span class=o>||</span> rm <span class=nv>$i</span> 
<span class=k>done</span></code></pre></td></tr></table></div></div><h2 id=learning-phase>Learning phase</h2><p>I&rsquo;ve had one &ldquo;issue&rdquo; during the learning phase. When I executed:</p><p><code>bazel-bin/tensorflow/examples/image_retraining/retrain --image_dir ~/flower_photos/</code></p><p>it failed with a message about <code>ModuleNotFoundError: No module named 'backports'</code>. I Googled and found the solution in this <a href=https://github.com/tensorflow/serving/issues/489#issuecomment-313671459>issue</a>. It is because I am using python3 and the tutorial has been written for python 2. No big deal.</p><p>At the end of the training (which took 12 minutes on a c4.2xlarge spot instance on AWS) I have two files that hold the previous information.</p><pre><code>...
2017-07-07 19:22:53.667219: Step 3990: Cross entropy = 0.111931
2017-07-07 19:22:53.728059: Step 3990: Validation accuracy = 93.0% (N=100)
2017-07-07 19:22:54.287266: Step 3999: Train accuracy = 98.0%
2017-07-07 19:22:54.287365: Step 3999: Cross entropy = 0.148188
2017-07-07 19:22:54.348603: Step 3999: Validation accuracy = 91.0% (N=100)
Final test accuracy = 92.7% (N=492)
Converted 2 variables to const ops.
...
</code></pre><p>And a trained graph with a label file that I can export and use elsewhere.</p><pre><code>(customenv) *[r1.2][~/sources/tensorflow]$ ls -lrth /tmp/output_*
-rw-rw-r-- 1 ubuntu ubuntu  47 Jul  7 19:22 /tmp/output_labels.txt
-rw-rw-r-- 1 ubuntu ubuntu 84M Jul  7 19:22 /tmp/output_graph.pb
</code></pre><p>I have followed the tutorial to <a href=https://www.tensorflow.org/tutorials/image_retraining#using_the_retrained_model>Use the retrained model</a> to make sure that everything was ok before using it with my own code.</p><h2 id=using-the-model-with-go>Using the model with go</h2><p>Tensorflow is coded in C++, but has some bindings for different languages. The most up-to-date is python, in which a lot of helper libraries are developed (see <a href=http://tflearn.org/getting_started/>tflearn</a> for example.
A binding for go exists, but it is only implementing the core library of tensorflow. Anyway, it is an excellent choice for applying a model.</p><p>The workflow is:</p><ul><li>read the exported model from the disk and create a new graph</li><li>read the label files and set the labels in an array of string</li><li>grab jpeg pictures from the webcam in jpeg (via v4l) in an endless for loop</li><li>Normalize the picture (see below) and create a tensor from the jpeg file.</li><li>Apply the inception model onto the Tensor and getting the <code>final_result</code></li><li>Extract the most important value from the output vector (the better probability) and display the corresponding label.</li></ul><p>I will only expose the trickiest parts.</p><h3 id=getting-the-pictures>Getting the pictures</h3><p>I use a wrapper around <code>v4l</code> in go called <a href=https://github.com/blackjack/webcam>go-webcam</a>. As my webcam has MJPEG capabilities, each frame is already in JPEG format.</p><p>I am applying the tensorflow model sequentially within the for loop. The problem is that it takes some time to process. And while it is processing the driver may buffer some pictures. Therefore I am totally losing the synchronism. My code may warn me that it has found a person too late.
To avoid this, I am using a non-blocking tick in a go channel within the loop. Therefore I do not process every single frame, but I process a frame every x milliseconds and I discard the rest.
I could have used a pool, but that would have add complexity for the example.</p><script type=application/javascript src="https://gist.github.com/owulveryck/1f3fc2366e5a35ab119633d57ad074b6.js?file=tick.go"></script><h3 id=normalizing-the-picture>Normalizing the picture</h3><p>The <a href=https://godoc.org/github.com/tensorflow/tensorflow/tensorflow/go>example described on the go package</a> is using an old inception implementation (actually version 5h which is older than the v3). Therefore it needs some adjustments. The function that produces a Tensorflow graph that will be used to normalize the picture didn&rsquo;t have the correct normalization values (those defined by the author of the inception v3 model)</p><p>Here is an extract from <a href=https://www.tensorflow.org/tutorials/image_recognition>Image Recognition</a>:</p><p><em>The model expects to get square 299x299 RGB images, so those are the <code>input_width</code> and <code>input_height</code> flags. We also need to scale the pixel values from integers that are between 0 and 255 to the floating point values that the graph operates on. We control the scaling with the <code>input_mean</code> and <code>input_std</code> flags: we first subtract <code>input_mean</code> from each pixel value, then divide it by <code>input_std</code>.</em></p><p><em>These values probably look somewhat magical, but they are just defined by the original model author based on what he/she wanted to use as input images for training. If you have a graph that you&rsquo;ve trained yourself, you&rsquo;ll just need to adjust the values to match whatever you used during your training process.</em></p><script type=application/javascript src="https://gist.github.com/owulveryck/1f3fc2366e5a35ab119633d57ad074b6.js?file=normalizationgraph.go"></script><p>Apart from that the rest of the code remains similar.</p><h1 id=conclusion>Conclusion</h1><h2 id=running-it-on-a-laptop>Running it on a laptop</h2><p>The program runs as expected at the rate of 2 images per seconds without overheating on a modern laptop. I have used it to monitor my house while I was on vacation. Every success was sent on an s3 bucket, so in case of the intrusion in my house, I would still have the pictures. I say that it has worked because the only pictures it has recorded were:</p><ul><li>me, leaving the house</li><li>me, entering the house 2 weeks later.</li></ul><p>You can find the full code on <a href=https://github.com/owulveryck/smarcctv>my github</a></p><h2 id=further-work>Further work</h2><h3 id=running-on-arm>Running on ARM</h3><p>I want to test it on a raspberry pi, so I have cross compiled the code for ARM with those commands but I didn&rsquo;t have time to test it yet:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code class=language-bash data-lang=bash><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-bash data-lang=bash><span class=c1># Download a tensorflow release for rpi:</span>
$ wget https://github.com/meinside/libtensorflow.so-raspberrypi/releases/download/v1.2.0/libtensorflow_v1.2.0_20170619.tgz
<span class=c1># Install the toolchain</span>
$ sudo apt install gcc-arm-linux-gnueabihf
<span class=c1># Compile it</span>
$ <span class=nb>export</span> <span class=nv>CC</span><span class=o>=</span>arm-linux-gnueabihf-gcc
$ <span class=nv>CC</span><span class=o>=</span>arm-linux-gnueabihf-gcc-5 <span class=nv>GOOS</span><span class=o>=</span>linux <span class=nv>GOARCH</span><span class=o>=</span>arm <span class=nv>GOARM</span><span class=o>=</span><span class=m>6</span> <span class=nv>CGO_ENABLED</span><span class=o>=</span><span class=m>1</span> go build  -o myprogram -ldflags<span class=o>=</span><span class=s2>&#34;-extld=</span><span class=nv>$CC</span><span class=s2>&#34;</span></code></pre></td></tr></table></div></div><h4 id=performances>Performances</h4><p>Inception is very good. But it requires a decent CPU (or even better a GPU). I could use another model called <a href=https://github.com/tensorflow/models/blob/master/slim/nets/mobilenet_v1.md>MobileNet</a> which is a <em>low latency, low power</em> model.
It has been <a href=https://research.googleblog.com/2017/06/mobilenets-open-source-models-for.html>opensourced</a> in June 2017. The tensorflow team has added the ability to retrain it the same way inception is (by retraining the last layer). It&rsquo;s worth a look.</p><h3 id=detecting-only-the-family>Detecting only the family</h3><p>As I explained in the beginning of the post, I want this system to trigger only if it detects someone that is not part of the family.
To do that I need to train the neuron network to classify classes such as:</p><ul><li>people</li><li>me</li><li>my wife</li><li>kid1</li><li>kid2</li></ul><p>To do so, I need training sets (labeled pictures) of my family. The best way to get it is to write a &ldquo;memory cortex&rdquo; to use it with my <a href=https://github.com/owulveryck/cortical>cortical</a> project as explained in my previous post: <a href=https://blog.owulveryck.info/2017/05/16/chrome-the-eye-of-the-cloud---computer-vision-with-deep-learning-and-only-2gb-of-ram.html>Chrome, the eye of the cloud - Computer vision with deep learning and only 2Gb of RAM</a>.</p></div><div class=post-copyright><p class=copyright-item><span class=item-title>Author</span>
<span class=item-content>Olivier Wulveryck</span></p><p class=copyright-item><span class=item-title>LastMod</span>
<span class=item-content>2017-07-07</span></p><p class=copyright-item><span class=item-title>License</span>
<span class=item-content><a rel="license noopener" href=https://creativecommons.org/licenses/by-nc-nd/4.0/ target=_blank>CC BY-NC-ND 4.0</a></span></p></div><footer class=post-footer><nav class=post-nav><a class=prev href=/2017/09/02/from-command-line-tools-to-microservices-the-example-of-hashicorp-tools-terraform-and-grpc/><i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="18" height="18"><path d="M691.908486 949.511495l75.369571-89.491197c10.963703-12.998035 10.285251-32.864502-1.499144-44.378743L479.499795 515.267417l277.93508-310.326815c11.338233-12.190647 11.035334-32.285311-.638543-44.850487l-80.46666-86.564541c-11.680017-12.583596-30.356378-12.893658-41.662889-.716314L257.233596 494.235404c-11.332093 12.183484-11.041474 32.266891.657986 44.844348l80.46666 86.564541c1.772366 1.910513 3.706415 3.533476 5.750981 4.877077l306.620399 321.703933C662.505829 963.726242 680.945807 962.528973 691.908486 949.511495z"/></svg></i><span class="prev-text nav-default">From command line tools to microservices - The example of Hashicorp tools (terraform) and gRPC</span>
<span class="prev-text nav-mobile">Prev</span></a>
<a class=next href=/2017/06/01/analyzing-a-parodic-trailer-nsfw-with-google-cloud-video-intelligence/><span class="next-text nav-default">Analyzing a parodic trailer (NSFW) with Google Cloud Video Intelligence</span>
<span class="prev-text nav-mobile">Next</span>
<i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="18" height="18"><path d="M332.091514 74.487481l-75.369571 89.491197c-10.963703 12.998035-10.285251 32.864502 1.499144 44.378743l286.278095 300.375162L266.565125 819.058374c-11.338233 12.190647-11.035334 32.285311.638543 44.850487l80.46666 86.564541c11.680017 12.583596 30.356378 12.893658 41.662889.716314l377.434212-421.426145c11.332093-12.183484 11.041474-32.266891-.657986-44.844348l-80.46666-86.564541c-1.772366-1.910513-3.706415-3.533476-5.750981-4.877077L373.270379 71.774697C361.493148 60.273758 343.054193 61.470003 332.091514 74.487481z"/></svg></i></a></nav></footer></article><div class=disqus-comment><div class=disqus-button id=load_disqus onclick=load_disqus()>Show Disqus Comments</div><div id=disqus_thread></div><script type=text/javascript>var disqus_config=function(){this.page.url="https://owulveryck.github.io/2017/07/07/a-smart-cctv-with-tensorflow-and-inception-on-a-rapsberry-pi/";};function load_disqus(){if(window.location.hostname==='localhost')return;var dsq=document.createElement('script');dsq.type='text/javascript';dsq.async=true;var disqus_shortname='owulveryck';dsq.src='//'+disqus_shortname+'.disqus.com/embed.js';(document.getElementsByTagName('head')[0]||document.getElementsByTagName('body')[0]).appendChild(dsq);$('#load_disqus').remove();};</script><noscript>Please enable JavaScript to view the
<a href=http://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript></div></div></div></main><footer id=footer class=footer><div class=icon-links><a href=https://twitter.com/owulveryck rel="me noopener" class=iconfont title=twitter target=_blank><svg class="icon" viewBox="0 0 1264 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="36" height="36"><path d="M1229.8616 18.043658c0 0-117.852626 63.135335-164.151872 67.344358-105.225559-164.151872-505.082682-92.598492-437.738325 223.078185C278.622548 312.675223 89.216542 47.506814 89.216542 47.506814s-117.852626 189.406006 75.762402 345.139833C127.097743 396.85567 55.544363 371.601535 55.544363 371.601535S26.081207 535.753407 253.368414 615.724832c-21.045112 29.463156-113.643603 8.418045-113.643603 8.418045s25.254134 143.10676 231.496229 180.987961c-143.10676 130.479693-387.230056 92.598492-370.393967 105.225559 206.242095 189.406006 1119.599946 231.496229 1128.01799-643.98042C1179.353331 249.539887 1263.533778 123.269217 1263.533778 123.269217s-130.479693 37.881201-138.897738 33.672179C1225.652577 98.015083 1229.8616 18.043658 1229.8616 18.043658"/></svg></a><a href=https://www.linkedin.com/in/olivierwulveryck/ rel="me noopener" class=iconfont title=linkedin target=_blank><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="33" height="33"><path d="M872.405333 872.618667H720.768v-237.610667c0-56.661333-1.152-129.578667-79.018667-129.578667-79.061333.0-91.136 61.653333-91.136 125.397334v241.792H398.976V384H544.64v66.602667h1.962667c20.352-38.4 69.845333-78.933333 143.786666-78.933334 153.642667.0 182.058667 101.12 182.058667 232.746667v268.202667zM227.712 317.141333a87.978667 87.978667.0 0 1-88.021333-88.106666A88.064 88.064.0 1 1 227.712 317.141333zm76.032 555.477334H151.68V384h152.064v488.618667zM948.266667.0h-872.704C33.792.0.0 33.024.0 73.770667v876.458666C0 991.018667 33.792 1024 75.562667 1024h872.576C989.866667 1024 1024 991.018667 1024 950.229333V73.770667C1024 33.024 989.866667.0 948.138667.0h.128z"/></svg></a><a href=http://github.com/owulveryck rel="me noopener" class=iconfont title=github target=_blank><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="36" height="36"><path d="M512 12.672c-282.88.0-512 229.248-512 512 0 226.261333 146.688 418.133333 350.08 485.76 25.6 4.821333 34.986667-11.008 34.986667-24.618667.0-12.16-.426667-44.373333-.64-87.04-142.421333 30.890667-172.458667-68.693333-172.458667-68.693333C188.672 770.986667 155.008 755.2 155.008 755.2c-46.378667-31.744 3.584-31.104 3.584-31.104 51.413333 3.584 78.421333 52.736 78.421333 52.736 45.653333 78.293333 119.850667 55.68 149.12 42.581333 4.608-33.109333 17.792-55.68 32.426667-68.48-113.706667-12.8-233.216-56.832-233.216-253.013333.0-55.893333 19.84-101.546667 52.693333-137.386667-5.76-12.928-23.04-64.981333 4.48-135.509333.0.0 42.88-13.738667 140.8 52.48 40.96-11.392 84.48-17.024 128-17.28 43.52.256 87.04 5.888 128 17.28 97.28-66.218667 140.16-52.48 140.16-52.48 27.52 70.528 10.24 122.581333 5.12 135.509333 32.64 35.84 52.48 81.493333 52.48 137.386667.0 196.693333-119.68 240-233.6 252.586667 17.92 15.36 34.56 46.762667 34.56 94.72.0 68.522667-.64 123.562667-.64 140.202666.0 13.44 8.96 29.44 35.2 24.32C877.44 942.592 1024 750.592 1024 524.672c0-282.752-229.248-512-512-512"/></svg></a><a href=https://owulveryck.github.io/index.xml rel="noopener alternate" type=application/rss&#43;xml class=iconfont title=rss target=_blank><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="30" height="30"><path d="M819.157333 1024C819.157333 574.592 449.408 204.8.0 204.8V0c561.706667.0 1024 462.293333 1024 1024H819.157333zM140.416 743.04a140.8 140.8.0 0 1 140.501333 140.586667A140.928 140.928.0 0 1 140.074667 1024C62.72 1024 0 961.109333.0 883.626667S62.933333 743.082667 140.416 743.04zM678.784 1024h-199.04c0-263.210667-216.533333-479.786667-479.744-479.786667V345.173333c372.352.0 678.784 306.517333 678.784 678.826667z"/></svg></a></div><div class=copyright><span class=power-by>Powered by <a class=hexo-link href=https://gohugo.io>Hugo</a></span>
<span class=division>|</span>
<span class=theme-info>Theme - <a class=theme-link href=https://github.com/xianmin/hugo-theme-jane>Jane</a></span>
<span class=copyright-year>&copy;
2015 -
2020
<span class=heart><i class=iconfont><svg class="icon" viewBox="0 0 1025 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="14" height="14"><path d="M1000.1 247.9c-15.5-37.3-37.6-70.6-65.7-98.9-54.4-54.8-125.8-85-201-85-85.7.0-166 39-221.4 107.4C456.6 103 376.3 64 290.6 64c-75.1.0-146.5 30.4-201.1 85.6-28.2 28.5-50.4 61.9-65.8 99.3-16 38.8-24 79.9-23.6 122.2.7 91.7 40.1 177.2 108.1 234.8 3.1 2.6 6 5.1 8.9 7.8 14.9 13.4 58 52.8 112.6 102.7 93.5 85.5 209.9 191.9 257.5 234.2 7 6.1 15.8 9.5 24.9 9.5 9.2.0 18.1-3.4 24.9-9.5 34.5-30.7 105.8-95.9 181.4-165 74.2-67.8 150.9-138 195.8-178.2 69.5-57.9 109.6-144.4 109.9-237.3.1-42.5-8-83.6-24-122.2z" fill="#8a8a8a"/></svg></i></span><span class=author>Olivier Wulveryck</span></span></div></footer><div class=back-to-top id=back-to-top><i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="35" height="35"><path d="M510.866688 227.694839 95.449397 629.218702h235.761562L329.15309 958.01517h362.40389L691.55698 628.188232l241.942331-3.089361L510.866688 227.694839zM63.840492 63.962777h894.052392v131.813095H63.840492V63.962777v0zm0 0"/></svg></i></div></div><script type=text/javascript src=/lib/jquery/jquery-3.2.1.min.js></script><script type=text/javascript src=/lib/slideout/slideout-1.0.1.min.js></script><script type=text/javascript src=/js/main.638251f4230630f0335d8c6748e53a96f94b72670920b60c09a56fdc8bece214.js integrity="sha256-Y4JR9CMGMPAzXYxnSOU6lvlLcmcJILYMCaVv3Ivs4hQ=" crossorigin=anonymous></script><script type=text/javascript src=/js/load-photoswipe.js></script><script type=text/javascript src=/lib/photoswipe/photoswipe.min.js></script><script type=text/javascript src=/lib/photoswipe/photoswipe-ui-default.min.js></script></body></html>