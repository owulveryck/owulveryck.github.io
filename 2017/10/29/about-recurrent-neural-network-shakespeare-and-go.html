<!doctype html><html lang=en itemscope itemtype=http://schema.org/WebPage><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><title>About Recurrent Neural Network, Shakespeare and GO - Unladen swallow - Olivier Wulveryck</title><meta name=renderer content=webkit><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta name=MobileOptimized content=width><meta name=HandheldFriendly content=true><meta name=applicable-device content=pc,mobile><meta name=theme-color content=#f8f5ec><meta name=msapplication-navbutton-color content=#f8f5ec><meta name=apple-mobile-web-app-capable content=yes><meta name=apple-mobile-web-app-status-bar-style content=#f8f5ec><meta name=mobile-web-app-capable content=yes><meta name=author content="Olivier Wulveryck"><meta name=description content="You may know how enthusiast I am about machine learning. A while ago I discovered recurrent neural networks. I have read that this 'tool' allow to predict the future! Is this a kind of magic? I have read a lot of stuffs about the 'unreasonable effectiveness' of this mechanism. The litteracy that gives deep explanation exists and is excellent. There is also plehtora of examples, but most of them are using python and a calcul framework. To fully undestand how things work (as I am not a data-scientist), I needed to write my own tool 'from scratch'. This is what this post is about: a more-or-less 'from scratch' implementation of a RNN in go that can be used to applied to a lot of examples"><meta name=keywords content=Go,Dev,IT><meta name=generator content="Hugo 0.58.3"><link rel=canonical href=https://owulveryck.github.io/2017/10/29/about-recurrent-neural-network-shakespeare-and-go.html><link rel=icon href=/favicon.ico><link rel=stylesheet href=/sass/jane.min.af20b78e95c84de86b00a0242a4a77bd2601700e1b250edf27537d957ac0041d.css integrity="sha256-ryC3jpXITehrAKAkKkp3vSYBcA4bJQ7fJ1N9lXrABB0=" media=screen crossorigin=anonymous><meta property=og:title content="About Recurrent Neural Network, Shakespeare and GO"><meta property=og:description content="You may know how enthusiast I am about machine learning. A while ago I discovered recurrent neural networks. I have read that this 'tool' allow to predict the future! Is this a kind of magic? I have read a lot of stuffs about the 'unreasonable effectiveness' of this mechanism. The litteracy that gives deep explanation exists and is excellent. There is also plehtora of examples, but most of them are using python and a calcul framework. To fully undestand how things work (as I am not a data-scientist), I needed to write my own tool 'from scratch'. This is what this post is about: a more-or-less 'from scratch' implementation of a RNN in go that can be used to applied to a lot of examples"><meta property=og:type content=article><meta property=og:url content=https://owulveryck.github.io/2017/10/29/about-recurrent-neural-network-shakespeare-and-go.html><meta property=og:image content=https://upload.wikimedia.org/wikipedia/en/6/63/Queen_A_Kind_Of_Magic.png><meta property=article:published_time content=2017-10-29T07:17:33+01:00><meta property=article:modified_time content=2017-10-29T07:17:33+01:00><meta itemprop=name content="About Recurrent Neural Network, Shakespeare and GO"><meta itemprop=description content="You may know how enthusiast I am about machine learning. A while ago I discovered recurrent neural networks. I have read that this 'tool' allow to predict the future! Is this a kind of magic? I have read a lot of stuffs about the 'unreasonable effectiveness' of this mechanism. The litteracy that gives deep explanation exists and is excellent. There is also plehtora of examples, but most of them are using python and a calcul framework. To fully undestand how things work (as I am not a data-scientist), I needed to write my own tool 'from scratch'. This is what this post is about: a more-or-less 'from scratch' implementation of a RNN in go that can be used to applied to a lot of examples"><meta itemprop=datePublished content=2017-10-29T07:17:33&#43;01:00><meta itemprop=dateModified content=2017-10-29T07:17:33&#43;01:00><meta itemprop=wordCount content=3004><meta itemprop=image content=https://upload.wikimedia.org/wikipedia/en/6/63/Queen_A_Kind_Of_Magic.png><meta itemprop=keywords content><meta name=twitter:card content=summary_large_image><meta name=twitter:image content=https://upload.wikimedia.org/wikipedia/en/6/63/Queen_A_Kind_Of_Magic.png><meta name=twitter:title content="About Recurrent Neural Network, Shakespeare and GO"><meta name=twitter:description content="You may know how enthusiast I am about machine learning. A while ago I discovered recurrent neural networks. I have read that this 'tool' allow to predict the future! Is this a kind of magic? I have read a lot of stuffs about the 'unreasonable effectiveness' of this mechanism. The litteracy that gives deep explanation exists and is excellent. There is also plehtora of examples, but most of them are using python and a calcul framework. To fully undestand how things work (as I am not a data-scientist), I needed to write my own tool 'from scratch'. This is what this post is about: a more-or-less 'from scratch' implementation of a RNN in go that can be used to applied to a lot of examples"><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script><script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]--><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-69673850-1','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script></head><body><div id=mobile-navbar class=mobile-navbar><div class=mobile-header-logo><a href=/ class=logo>owulveryck's blog</a></div><div class=mobile-navbar-icon><span></span><span></span><span></span></div></div><nav id=mobile-menu class="mobile-menu slideout-menu"><ul class=mobile-menu-list><li class=mobile-menu-item><a class=menu-item-link href=https://owulveryck.github.io/>This is Home</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://owulveryck.github.io/post/>Archives</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://owulveryck.github.io/tags/>Tags</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://owulveryck.github.io/categories/>Categories</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://owulveryck.github.io/about.html>About</a></li></ul></nav><link rel=stylesheet href=/lib/photoswipe/photoswipe.min.css><link rel=stylesheet href=/lib/photoswipe/default-skin/default-skin.min.css><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><header id=header class="header container"><div class=logo-wrapper><a href=/ class=logo>owulveryck's blog</a></div><nav class=site-navbar><ul id=menu class=menu><li class=menu-item><a class=menu-item-link href=https://owulveryck.github.io/>This is Home</a></li><li class=menu-item><a class=menu-item-link href=https://owulveryck.github.io/post/>Archives</a></li><li class=menu-item><a class=menu-item-link href=https://owulveryck.github.io/tags/>Tags</a></li><li class=menu-item><a class=menu-item-link href=https://owulveryck.github.io/categories/>Categories</a></li><li class=menu-item><a class=menu-item-link href=https://owulveryck.github.io/about.html>About</a></li></ul></nav></header><div id=mobile-panel><main id=main class="main bg-llight"><div class=content-wrapper><div id=content class="content container"><article class="post bg-white"><header class=post-header><h1 class=post-title>About Recurrent Neural Network, Shakespeare and GO</h1><div class=post-meta><time datetime=2017-10-29 class=post-time>2017-10-29</time></div></header><div class=post-toc id=post-toc><h2 class=post-toc-title>Table of Contents</h2><div class=post-toc-content><nav id=TableOfContents><ul><li><a href=#shakespeare-and-i-encounter-of-the-third-type>Shakespeare and I, encounter of the third type</a></li><li><a href=#first-part-the-rnn-and-i-first-episode-of-a-time-serie>First part: The RNN and I, first episode of a time-serie</a><ul><li><ul><li><a href=#experimenting-with-rnn>Experimenting with RNN</a></li><li><a href=#the-initial-example>The initial example</a></li><li><a href=#how-does-it-work>How does it work?</a></li><li><a href=#a-classification-problem>A classification problem</a></li></ul></li></ul></li><li><a href=#second-part-let-s-geek>Second part: Let&rsquo;s geek</a><ul><li><a href=#the-rnn-package>The rnn package</a><ul><li><a href=#the-rnn-object>The RNN object</a></li><li><a href=#rnn-s-step>RNN&rsquo;s step</a></li><li><a href=#the-train-method>The Train method</a></li><li><a href=#forward-processing>Forward processing</a></li><li><a href=#back-propagation-through-time>Back propagation through time</a></li><li><a href=#adapting-the-parameters-via-adagrad>Adapting the parameters via &ldquo;AdaGrad&rdquo;</a></li><li><a href=#prediction>Prediction</a></li></ul></li><li><a href=#enhancement-of-the-tool-implementing-codecs>Enhancement of the tool: implementing codecs</a><ul><li><a href=#the-codec-interface>The codec interface</a></li><li><a href=#the-char-implementation-of-the-codec-interface>The char implementation of the codec interface</a></li></ul></li><li><a href=#the-main-tool>The main tool</a></li></ul></li><li><a href=#conclusion>Conclusion</a></li></ul></nav></div></div><div class=post-content><h1 id=shakespeare-and-i-encounter-of-the-third-type>Shakespeare and I, encounter of the third type</h1><p>A couple of months ago, I attended the Google Cloud Next 17 event in London.
Among the talks about SRE, and keynotes, I had the chance to listen to Martin Gorner&rsquo;s excellent introduction: <a href="https://www.youtube.com/watch?v=fTUwdXUFfI8">TensorFlow and Deep Learning without a PhD, Part 2</a>. If you don&rsquo;t want to look at the video, here is a quick summary:</p><p><em>a 100 of lines of python are reading all Shakespeare&rsquo;s plays; it learns his style, and then generates a brand new play from scratch.</em></p><p>Of course, when you are not a data-scientist (and I am not), this looks pretty amazing (and a bit magical).</p><p>Back home, I told my friends how amazing it was. I downloaded the code from <a href=https://github.com/martin-gorner/tensorflow-rnn-shakespeare>github</a>, installed tensorflow, and played my Shakespeare to show them.
In essence, here is what they told me:</p><ul><li><em>&ldquo;Amazing, and you know how this works?</em></li><li><em>Well&hellip;&rdquo;</em> let&rsquo;s be honest, I had only a vague idea.</li></ul><p>It was about something called &ldquo;Recurrent Neural Networks&rdquo; (aka <em>RNN</em>).
I dived into the internet&hellip; 100 lines of python shouldn&rsquo;t be hard to understand. And to reproduce ?
Where they?</p><p>Actually, it took me months to be able to write this post, without any previous knowledge, it was not that easy.</p><p>This is the reason why I finally wrote this article. I want to be sure that I have understood the structure and the possibilities offered by recurrent neural networks.
I also wanted to see whether building a RNN powered tool was doable easily.</p><p>This document is divided into two parts:</p><ul><li>the first part is about recurrent neural networks in general;</li><li>the second part is about a toy I made in GO to play with RNNs.</li></ul><p>The goal of this text is not to talk about the mathematics behind the neural networks.
Of course, I may talk about vectors, but I will not talk about non-linearity or hyperbolic functions.</p><p>I hope you will be enthusiastic, as much as I am.</p><p>Anyway, do not hesitate to give me any feedback or suggestion that may improve my work.</p><h1 id=first-part-the-rnn-and-i-first-episode-of-a-time-serie>First part: The RNN and I, first episode of a time-serie</h1><p>The Web is full of resources about machine learning. You can easily find great articles, very well illustrated about neural networks.
I&rsquo;ve read a lot&hellip;</p><p>The more I learn, the more excited I get.</p><p>For example, I&rsquo;ve discovered that RNN could, by nature, predict time series
(cf <a href=http://dataconomy.com/2017/05/how-to-do-time-series-prediction-using-rnns-tensorflow-and-cloud-ml-engine/>how to do time series prediction using RNNs, Tensorflow and Cloud ML engine</a>).</p><ul><li><em>&ldquo;Wait, does it mean that it can predict the future?</em></li><li>Well, kind of&hellip;&rdquo;</li></ul><p>It is still in the area of &ldquo;supervised learning&rdquo;.</p><p>Therefore, the algorithm learns events. Based on this, the algorithm can predict what may come next; but only if it is something it has already seen.
Let&rsquo;s take an example. Consider a lottery game (everybody ask me about this):</p><p>To win, your ticket&rsquo;s sequence of numbers must be identical to the one that will be chosen, randomly, at the next draw.
If RNN can predict the future, it should, basically, be able to predict it.</p><p>The RNN must learn about the sequences to apply its knowledge (and become a fortune-teller). So If every week the draw is made of &ldquo;1 2 3 4 5 6&rdquo;, the RNN will learn, and tell that the next draw will be: &ldquo;1 2 3 4 5 6&rdquo;.</p><p>Obviously this is useless; now let&rsquo;s consider a more complex sequence:</p><table><thead><tr><th>Week</th><th>sequence</th></tr></thead><tbody><tr><td>1</td><td>1 2 3 4 5 6</td></tr><tr><td>2</td><td>2 3 4 5 6 1</td></tr><tr><td>3</td><td>3 4 5 6 1 2</td></tr><tr><td>4</td><td>4 5 6 1 2 3</td></tr><tr><td>5</td><td>5 6 1 2 3 4</td></tr><tr><td>6</td><td>6 1 2 3 4 5</td></tr><tr><td>7</td><td>1 2 3 4 5 6</td></tr><tr><td>8</td><td>? ? ? ? ? ?</td></tr></tbody></table><p>Question: What will be the winning sequence of week 8?</p><p>&ldquo;2 3 4 5 6 1&rdquo;. Cool, you are rich!
How did you do it? You memorized the sequence.</p><p>RNN does exactly the same.</p><ul><li>&ldquo;So, it <strong>can</strong> predict the next lottery?</li><li>No, because there is no sequence in the lottery, it is pure randomness&rdquo;.</li></ul><p>In other word, there is no &ldquo;<em>recurrence</em>&rdquo; in the drawing. Therefore, &ldquo;<em>recurrent</em>&rdquo; neural networks cannot be applied.</p><p>Anyway, beside the lottery, a lot of events are, in essence, recurrents.
The point is that the recurrency model is usually not obvious and not that easy to detect. This is the famous &ldquo;feeling&rdquo; of the experts.</p><p>For example, you may have already heard:</p><ul><li>&ldquo;Will the system crash?</li><li>Based of what I see and what I know, I [don&rsquo;t] think so&rdquo;.</li></ul><p>or,</p><ul><li>&ldquo;Will the sales increase on Sunday?</li><li>Regarding the current market situation and on my experience, it may&rdquo;.</li></ul><p>This is where a RNN could shine and enhance our professional lives.</p><p>In a pure IT context, for example, you have failures &ldquo;every now-and-then&rdquo;. Even if you don&rsquo;t find the root cause, it could be useful to predict the next failure.
If you have enough data about the past failures, the RNN could learn the pattern, and tell you when the next failure will occur.</p><hr><p><center><blockquote class=twitter-tweet><p lang=en dir=ltr>Gartner&#39;s <a href="https://twitter.com/CarlieIdoine?ref_src=twsrc%5Etfw">@CarlieIdoine</a> explains how &#39;citizen&#39; <a href="https://twitter.com/hashtag/data?src=hash&amp;ref_src=twsrc%5Etfw">#data</a> scientists in your organization can make pervasive <a href="https://twitter.com/hashtag/analytics?src=hash&amp;ref_src=twsrc%5Etfw">#analytics</a> a reality <a href="https://twitter.com/hashtag/GartnerDA?src=hash&amp;ref_src=twsrc%5Etfw">#GartnerDA</a> <a href=https://t.co/iRdm2vNsuJ>pic.twitter.com/iRdm2vNsuJ</a></p>&mdash; Gartner for IT (@Gartner_IT) <a href="https://twitter.com/Gartner_IT/status/844561153229541376?ref_src=twsrc%5Etfw">March 22, 2017</a></blockquote><script async src=https://platform.twitter.com/widgets.js></script></center></p><hr><h3 id=experimenting-with-rnn>Experimenting with RNN</h3><p>I needed a simple tool to do experimentations.
A huge majority of articles about machine learning are using Python and a framework (such as Tensorflow).
To me, it has two major drawbacks:</p><ul><li>I need to fully understand how to use the framework;</li><li>as it is Python related (and I am not fluent in Python), building <strong>and deploying</strong> efficient tools could take some time.</li></ul><p>Let&rsquo;s be more specific about the second point.</p><p>I have seen a lot of samples that could do very beautiful stuffs based on fake data.
Playing with everyday data usually implies to rewrite the tool, from scratch&hellip;</p><p>Therefore, I have decided to implement a RNN engine from scratch, in GO (I am &ldquo;fluent&rdquo; in go, that have save me days of debugging).
The goal is simple: to understand how RNN works.</p><p><em>&ldquo;Whatever is well conceived is clearly said, and the words to say it flow with ease&rdquo;.</em></p><p><em>&ldquo;Ce que l&rsquo;on conçoit bien s&rsquo;énonce clairement, et les mots pour le dire arrivent aisément&rdquo;.</em></p><p align=right><i>Nicolas Boileau</i></p><h3 id=the-initial-example>The initial example</h3><p>The following example is basically an adaptation of Andrej Karpathy&rsquo;s post: <a href=http://karpathy.github.io/2015/05/21/rnn-effectiveness/>The Unreasonable Effectiveness of Recurrent Neural Networks</a>. I strongly encourage you to read it.</p><p>Anyway, I will give you a couple of explanations of the principle.
The goal is to write and train a RNN with a certain amount of text data.</p><p>Then, once the RNN is trained, we ask the tool to generate a new text based on what it has learned, character by character.</p><h3 id=how-does-it-work>How does it work?</h3><p>Consider the &ldquo;HELLO&rdquo; example as described in Karpathy&rsquo;s post.
The vocabulary of the example is composed of 4 letters: <code>H</code>, <code>E</code>, <code>L</code> and <code>O</code>.</p><p>The principle is to train the RNN in order to make prediction for the next letter.</p><p>Therefore, if I give an <code>H</code> as input the fully trained RNN, it will return an <code>E</code>,</p><p>Then, the <code>E</code> will become the input, and the output will be an <code>L</code>.</p><p>This <code>L</code> will become the new input. Here is a difficulty: after an <code>L</code>, there can be:</p><ul><li>another <code>L</code>,</li><li>or an <code>O</code>.</li></ul><p>This is what make RNN suitable for this case: RNN have a memory!
Then, it will most probably choose a second <code>L</code>, based, not only on the last letter, but also on the previous <code>H</code> and <code>E</code> it has seen.</p><p>Correctly trained, the RNN should be able to produce an <code>O</code>.</p><h3 id=a-classification-problem>A classification problem</h3><p>In practice, this is a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification problem</a>; every letter in the alphabet is a class.</p><p>Given a sequence of letters as input, the mechanism should predict which class it belongs to. This class represents the next letter to be displayed.</p><p>For example:</p><ul><li><code>h</code> belongs to class <code>e</code></li><li><code>h e</code> belongs to class <code>l</code></li><li><code>h e l</code> also belongs to classe <code>l</code></li><li><code>h e l l</code> belongs to class <code>o</code></li></ul><p>The network will compute, for each class, a probability based on the input and the context.
So, every letter will be assigned a value between 0 and 1 by the algorithm.</p><p>If we formalize that in an array, the ideal situation would be:</p><p><html><table border=1 align=center><tr><th>context</th><th>input</th><th>Probability of class H</th><th>Probability of class E</th><th>Probability of class L</th><th>Probability of class O</th></tr><tr><td></td><td>H</td><td>0</td><td>1</td><td>0</td><td>0</td></tr><tr><td>H</td><td>E</td><td>0</td><td>0</td><td>1</td><td>0</td></tr><tr><td>H e</td><td>L</td><td>0</td><td>0</td><td>1</td><td>0</td></tr><tr><td>H E L</td><td>L</td><td>0</td><td>0</td><td>0</td><td>1</td></tr></table></html></p><p>In practice, we may have something slightly different (this is an example, do not try to interpret the values):</p><p><html><table border=1 align=center><tr><th>context</th><th>input</th><th>Probability of class H</th><th>Probability of class E</th><th>Probability of class L</th><th>Probability of class O</th></tr><tr><td></td><td>H</td><td>0.1</td><td>0.8</td><td>0.05</td><td>0.05</td></tr><tr><td>H</td><td>E</td><td>0.1</td><td>0.07</td><td>0.8</td><td>0.03</td></tr><tr><td>H E</td><td>L</td><td>0.05</td><td>0.05</td><td>0.5</td><td>0.4</td></tr><tr><td>H E L</td><td>L</td><td>0.05</td><td>0.05</td><td>0.4</td><td>0.5</td></tr></table></html></p><p>We have encoded the output into an array. In mathematics, such array is called a vector.</p><p>On the same principle, we can encode the input letters into a <em>1-of-k</em> vector (1 in the cell corresponding to character, 0 elsewhere).</p><pre> 
<code>  
    h e l o
h = 1 0 0 0
e = 0 1 0 0
l = 0 0 1 0
o = 0 0 0 1
</code>
</pre><p>The purpose of the prediction is to apply a mathematical function to an input vector in order to produce an output vector (composed of probabilities).
The next character should be chosen accordingly.</p><p>The RNN does not know natively an equation able to predict the correct values but a mathematical model (a composition of mathematical function). This model contains a lot of parameters or variables.</p><p>With proper values, those parameters applied to the mathematical model should allow to compute the correct vector.</p><p>Finding the correct parameters is called <em>the training process</em>.</p><p>The RNN, fed with a lot of data and their expected output, will adjust its internal parameters.</p><p>At each step, the difference between the output and the expected result is evaluated; it is call <em>the loss</em>.</p><p>The purpose of the adaptation is to reduce the loss at every step.</p><h1 id=second-part-let-s-geek>Second part: Let&rsquo;s geek</h1><p>From now on, let&rsquo;s talk about the implementation; feel free to skip this part and jump straight to the conclusion if your are not interested in coding.</p><p>I want to create a tool able to generate a Shakespeare play as described in Karpathy&rsquo;s blog post.
His implementation in Python is <a href=https://gist.github.com/karpathy/d4dee566867f8291f086>here</a>; you can find mine <a href=https://github.com/owulveryck/min-char-rnn>here</a>.</p><p><strong>edit</strong> At first, it was a simple transcript from Python to GO, but the tool has been enhanced. It is now a more generic tool able to use RNN as a processing unit. It&rsquo;s pluggable to any code able to encode and decode a sequence of bytes into a vector.</p><h2 id=the-rnn-package>The rnn package</h2><p>I have created a separate package for two reasons:</p><ul><li>to fully understand what is related to the RNN;</li><li>to see what is related to the example about character recognition.</li></ul><p>For the same reasons, I have tried to keep parameters as private as possible within the objects.</p><p>I am using the <a href=https://godoc.org/github.com/gonum/matrix/mat64><code>mat64.Dense</code></a> structure to represent the matrices and simple <code>[]float64</code> elements for column vectors
(for more info: <a href=https://blog.golang.org/go-slices-usage-and-internals#clices>Go Slices: usage and internals</a>).</p><h3 id=the-rnn-object>The RNN object</h3><p>The RNN structure holds the three matrices representing the weights to be adapted:</p><ul><li>Wxh</li><li>Whh</li><li>Why</li></ul><p>On top of that, the RNN stores two &ldquo;vectors&rdquo; for the <a href=https://stackoverflow.com/questions/2480650/role-of-bias-in-neural-networks>bias</a>. One for the hidden layer, the other for the output layer.</p><p>The hidden vector is not stored within the structure. Only the last hidden vector evaluated in the process of feedforward/backpropagation is stored.</p><p>Not storing the hidden vector within the structure allows to use the same &ldquo;step&rdquo; function in the sampling process as well as the training process.</p><h3 id=rnn-s-step>RNN&rsquo;s step</h3><p>RNN&rsquo;s step method is the proper implementation of the neural network as described by <em>Karpathy</em>.
As explained before, the hidden state is not part of the RNN structure. It is an output of the step function:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code class=language-go data-lang=go><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-go data-lang=go><span class=kd>func</span> <span class=p>(</span><span class=nx>rnn</span> <span class=o>*</span><span class=nx>RNN</span><span class=p>)</span> <span class=nf>step</span><span class=p>(</span><span class=nx>x</span><span class=p>,</span> <span class=nx>hprev</span> <span class=p>[]</span><span class=kt>float64</span><span class=p>)</span> <span class=p>(</span><span class=nx>y</span><span class=p>,</span> <span class=nx>h</span> <span class=p>[]</span><span class=kt>float64</span><span class=p>)</span> <span class=p>{</span>
	<span class=nx>h</span> <span class=p>=</span> <span class=nf>tanh</span><span class=p>(</span>
		<span class=nf>add</span><span class=p>(</span>
			<span class=nf>dot</span><span class=p>(</span><span class=nx>rnn</span><span class=p>.</span><span class=nx>wxh</span><span class=p>,</span> <span class=nx>x</span><span class=p>),</span>
			<span class=nf>dot</span><span class=p>(</span><span class=nx>rnn</span><span class=p>.</span><span class=nx>whh</span><span class=p>,</span> <span class=nx>hprev</span><span class=p>),</span>
			<span class=nx>rnn</span><span class=p>.</span><span class=nx>bh</span><span class=p>,</span>
		<span class=p>))</span>
	<span class=nx>y</span> <span class=p>=</span> <span class=nf>add</span><span class=p>(</span>
		<span class=nf>dot</span><span class=p>(</span><span class=nx>rnn</span><span class=p>.</span><span class=nx>why</span><span class=p>,</span> <span class=nx>h</span><span class=p>),</span>
		<span class=nx>rnn</span><span class=p>.</span><span class=nx>by</span><span class=p>)</span>
	<span class=k>return</span>
<span class=p>}</span></code></pre></td></tr></table></div></div><p>You see here that the step function of my RNN takes two vectors as input:</p><ul><li>a vector representing the currently evaluated item (remember, it is the representation of the <code>H</code>, <code>E</code>, <code>L</code> and <code>O</code> in the previous example),</li><li>a hidden vector that is the memory of the passed elements.</li></ul><p>It returns two vectors:</p><ul><li>the evaluated output in term of vector (again it is the representation of the <code>H</code>, <code>E</code>, <code>L</code> and <code>O</code>),</li><li>a new and updated hidden vector.</li></ul><p><em>Note</em> : For clarity, I have declared a couple of math helpers such as <code>dot</code>, <code>tanh</code> and <code>add</code> that are out of the scope of the explanation.</p><h3 id=the-train-method>The Train method</h3><p>This method is returning two channels and triggers a goroutine that does the job of training.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code class=language-go data-lang=go><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-go data-lang=go><span class=kd>func</span> <span class=p>(</span><span class=nx>rnn</span> <span class=o>*</span><span class=nx>RNN</span><span class=p>)</span> <span class=nf>Train</span><span class=p>()</span> <span class=p>(</span><span class=kd>chan</span><span class=o>&lt;-</span> <span class=nx>TrainingSet</span><span class=p>,</span> <span class=kd>chan</span> <span class=kt>float64</span><span class=p>)</span> <span class=p>{</span>
    <span class=o>...</span>
<span class=p>}</span></code></pre></td></tr></table></div></div><p>The first channel is a feeding channel for the RNN. It receives a <code>TrainingSet</code> that is composed of:</p><ul><li>an input vector</li><li>a target vector</li></ul><p>The goroutine will read the channel, and get all the training data.
It will evaluates the input of the training set and use the target to adapt the parameters.</p><p>The second channel is a non blocking channel. It is used to transfer the loss evaluated at each pass for information purpose.</p><h3 id=forward-processing>Forward processing</h3><p>The forward processing takes a batch of inputs (an array of array) and a sequence of outputs.
It runs the step as many times as needed and stores the hidden vectors in a temporary array. Then the values are used for the back propagation.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code class=language-go data-lang=go><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-go data-lang=go><span class=kd>func</span> <span class=p>(</span><span class=nx>rnn</span> <span class=o>*</span><span class=nx>RNN</span><span class=p>)</span> <span class=nf>forwardPass</span><span class=p>(</span><span class=nx>xs</span> <span class=p>[][]</span><span class=kt>float64</span><span class=p>,</span> <span class=nx>hprev</span> <span class=p>[]</span><span class=kt>float64</span><span class=p>)</span> <span class=p>(</span><span class=nx>ys</span><span class=p>,</span> <span class=nx>hs</span> <span class=p>[][]</span><span class=kt>float64</span><span class=p>)</span> <span class=p>{</span>
    <span class=o>...</span>
<span class=p>}</span></code></pre></td></tr></table></div></div><h3 id=back-propagation-through-time>Back propagation through time</h3><p>The back propagation is evaluating the gradient. Once the evaluation is done, parameters can be adapted according to the computed gradients.</p><h3 id=adapting-the-parameters-via-adagrad>Adapting the parameters via &ldquo;AdaGrad&rdquo;</h3><p>The method used by Karpathy is the Adaptive gradient.
This one needs a memory; therefore I have declared a new object for the adagrad with a simple Apply method.
The <code>apply</code> method of the <code>adagrad</code> object takes the neural network as a parameter and the previously evaluated gradients.</p><p>Once this process is done, the RNN is trained and usable.</p><h3 id=prediction>Prediction</h3><p>I have implemented a <code>Predict</code> method that applies the same method. It starts with an empty memory (the hidden vector is zeroed), takes a sample text as input and generate the output without evaluating the gradient nor adapting the parameters.</p><p>This RNN implementation is enough to generate the Shakespeare.</p><h2 id=enhancement-of-the-tool-implementing-codecs>Enhancement of the tool: implementing codecs</h2><p>In order to work with any character (= any symbol), the best way to <em>GO</em> is to use the concept of <a href=https://blog.golang.org/strings>rune</a>.
The first implementation of the min-char-rnn I made was using this package. It was simply implementing and a couple of functions to 1-of-k encode and decode the rune, one at a time.</p><p>It was working as expected, but I was stuck within the character based neural network.</p><p>As explained before, the RNN package is working with vectors, and have no knowledge of characters, pictures, bytes or whatever.</p><p>So to continue with this level of abstraction, I have declared a codec interface.</p><p>Therefore, the character based example is simply an implementation that fulfills this interface.</p><h3 id=the-codec-interface>The codec interface</h3><p>The codec interface describes the required methods any object must implement in order to use the RNN.</p><p>It allows any implementation to use the RNN (imagine a log parser, an image encoder/decoder, a webservice [<em>insert whatever fancy idea here</em>]&hellip;)</p><p>The most important methods of the interface are:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code class=language-go data-lang=go><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-go data-lang=go><span class=nf>Decode</span><span class=p>([][]</span><span class=kt>float64</span><span class=p>)</span> <span class=nx>io</span><span class=p>.</span><span class=nx>Reader</span>
<span class=nf>Encode</span><span class=p>(</span><span class=nx>io</span><span class=p>.</span><span class=nx>Reader</span><span class=p>)</span> <span class=p>[][]</span><span class=kt>float64</span></code></pre></td></tr></table></div></div><p>Those methods are dealing with arrays of vectors on one side, and with <code>io.Reader</code> on the other side.
Therefore, it can use any input type, from a text representation to a data flow over the network (and if you are <em>gopher</em>, you know how cool <code>io.Reader</code> are!)</p><p>The other methods are simply helper functions useful to train the network. I have also chosen to add a post processing method:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code class=language-go data-lang=go><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-go data-lang=go><span class=nf>ApplyDist</span><span class=p>([]</span><span class=kt>float64</span><span class=p>)</span> <span class=p>[]</span><span class=kt>float64</span></code></pre></td></tr></table></div></div><p>This method is a post processing of the output vector. Actually, the returned vector is made of normalized probabilities.
In a classification mechanism, one element must be chosen.
Obviously, the algorithm should choose the one with the best probability.</p><p>But, in the case of the char example, we can add some randomness by selecting the output class according to a certain distribution.
I have implemented a <a href=https://godoc.org/github.com/gonum/stat/distuv#Categorical>Bernouilli distribution</a> for the char codec. It is selectable by setting <code>CHAR_CODEC_CHOICE=soft</code> in the environment).</p><p>This function also let the possibility to get the raw normalized probabilities by implementing a no-ops func.</p><p><em>Note</em> : This interface should be reworked, because <em>Pike</em> loves the one-function-interfaces, and <em>Pike</em> knows!</p><h3 id=the-char-implementation-of-the-codec-interface>The char implementation of the codec interface</h3><p>As explained before, the char implementation consists of a couple of methods that reads a file and encode it.
It can also decode a generated output.</p><p>It is that simple. It also serve as an example for whatever new codec implementation.</p><h2 id=the-main-tool>The main tool</h2><p>The main tool is just the glue between all the packages.
It can be used to train the network or to generate an output. The parameters are tweakable via environment variables (actually each package deals with their own environment variables).</p><p>you can find all the code on my <a href=https://github.com/owulveryck/min-char-rnn>gitHub</a>. It needs tweaking and deep testing though.
I have also uploaded a binary distribution and a pre-trained model (I have implemented a backup and restore mechanism in order to use a pre-trained model).</p><h1 id=conclusion>Conclusion</h1><p>I have now understood the principle of RNN. It offers me a lot of opportunities for future work.</p><p>I would like to develop some sample tools that could be useful in my everyday life.</p><p>Why not writing a codec that can parse the log files of an application. The output would be an encoded status of the health of the application (red, orange or green).</p><p>With the correct data about when warnings or failures occurred, it should able to predict the next failure&hellip; before it happens.</p></div><div class=post-copyright><p class=copyright-item><span class=item-title>Author</span>
<span class=item-content>Olivier Wulveryck</span></p><p class=copyright-item><span class=item-title>LastMod</span>
<span class=item-content>2017-10-29</span></p><p class=copyright-item><span class=item-title>License</span>
<span class=item-content><a rel="license noopener" href=https://creativecommons.org/licenses/by-nc-nd/4.0/ target=_blank>CC BY-NC-ND 4.0</a></span></p></div><footer class=post-footer><nav class=post-nav><a class=prev href=/2017/12/18/parsing-mathematical-equation-to-generate-computation-graphs-first-step-from-software-1.0-to-2.0-in-go.html><i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="18" height="18"><path d="M691.908486 949.511495l75.369571-89.491197c10.963703-12.998035 10.285251-32.864502-1.499144-44.378743L479.499795 515.267417l277.93508-310.326815c11.338233-12.190647 11.035334-32.285311-.638543-44.850487l-80.46666-86.564541c-11.680017-12.583596-30.356378-12.893658-41.662889-.716314L257.233596 494.235404c-11.332093 12.183484-11.041474 32.266891.657986 44.844348l80.46666 86.564541c1.772366 1.910513 3.706415 3.533476 5.750981 4.877077l306.620399 321.703933C662.505829 963.726242 680.945807 962.528973 691.908486 949.511495z"/></svg></i><span class="prev-text nav-default">Parsing mathematical equation to generate computation graphs - First step from software 1.0 to 2.0 in go</span>
<span class="prev-text nav-mobile">Prev</span></a>
<a class=next href=/2017/09/12/terraform-is-hip...-introducing-nhite.html><span class="next-text nav-default">Terraform is hip... Introducing Nhite</span>
<span class="prev-text nav-mobile">Next</span>
<i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="18" height="18"><path d="M332.091514 74.487481l-75.369571 89.491197c-10.963703 12.998035-10.285251 32.864502 1.499144 44.378743l286.278095 300.375162L266.565125 819.058374c-11.338233 12.190647-11.035334 32.285311.638543 44.850487l80.46666 86.564541c11.680017 12.583596 30.356378 12.893658 41.662889.716314l377.434212-421.426145c11.332093-12.183484 11.041474-32.266891-.657986-44.844348l-80.46666-86.564541c-1.772366-1.910513-3.706415-3.533476-5.750981-4.877077L373.270379 71.774697C361.493148 60.273758 343.054193 61.470003 332.091514 74.487481z"/></svg></i></a></nav></footer></article><div class=disqus-comment><div class=disqus-button id=load_disqus onclick=load_disqus()>Show Disqus Comments</div><div id=disqus_thread></div><script type=text/javascript>var disqus_config=function(){this.page.url="https://owulveryck.github.io/2017/10/29/about-recurrent-neural-network-shakespeare-and-go.html";};function load_disqus(){if(window.location.hostname==='localhost')return;var dsq=document.createElement('script');dsq.type='text/javascript';dsq.async=true;var disqus_shortname='owulveryck';dsq.src='//'+disqus_shortname+'.disqus.com/embed.js';(document.getElementsByTagName('head')[0]||document.getElementsByTagName('body')[0]).appendChild(dsq);$('#load_disqus').remove();};</script><noscript>Please enable JavaScript to view the
<a href=http://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript></div></div></div></main><footer id=footer class=footer><div class=icon-links><a href=https://twitter.com/owulveryck rel="me noopener" class=iconfont title=twitter target=_blank><svg class="icon" viewBox="0 0 1264 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="36" height="36"><path d="M1229.8616 18.043658c0 0-117.852626 63.135335-164.151872 67.344358-105.225559-164.151872-505.082682-92.598492-437.738325 223.078185C278.622548 312.675223 89.216542 47.506814 89.216542 47.506814s-117.852626 189.406006 75.762402 345.139833C127.097743 396.85567 55.544363 371.601535 55.544363 371.601535S26.081207 535.753407 253.368414 615.724832c-21.045112 29.463156-113.643603 8.418045-113.643603 8.418045s25.254134 143.10676 231.496229 180.987961c-143.10676 130.479693-387.230056 92.598492-370.393967 105.225559 206.242095 189.406006 1119.599946 231.496229 1128.01799-643.98042C1179.353331 249.539887 1263.533778 123.269217 1263.533778 123.269217s-130.479693 37.881201-138.897738 33.672179C1225.652577 98.015083 1229.8616 18.043658 1229.8616 18.043658"/></svg></a><a href=https://www.linkedin.com/in/olivierwulveryck/ rel="me noopener" class=iconfont title=linkedin target=_blank><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="33" height="33"><path d="M872.405333 872.618667H720.768v-237.610667c0-56.661333-1.152-129.578667-79.018667-129.578667-79.061333.0-91.136 61.653333-91.136 125.397334v241.792H398.976V384H544.64v66.602667h1.962667c20.352-38.4 69.845333-78.933333 143.786666-78.933334 153.642667.0 182.058667 101.12 182.058667 232.746667v268.202667zM227.712 317.141333a87.978667 87.978667.0 0 1-88.021333-88.106666A88.064 88.064.0 1 1 227.712 317.141333zm76.032 555.477334H151.68V384h152.064v488.618667zM948.266667.0h-872.704C33.792.0.0 33.024.0 73.770667v876.458666C0 991.018667 33.792 1024 75.562667 1024h872.576C989.866667 1024 1024 991.018667 1024 950.229333V73.770667C1024 33.024 989.866667.0 948.138667.0h.128z"/></svg></a><a href=http://github.com/owulveryck rel="me noopener" class=iconfont title=github target=_blank><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="36" height="36"><path d="M512 12.672c-282.88.0-512 229.248-512 512 0 226.261333 146.688 418.133333 350.08 485.76 25.6 4.821333 34.986667-11.008 34.986667-24.618667.0-12.16-.426667-44.373333-.64-87.04-142.421333 30.890667-172.458667-68.693333-172.458667-68.693333C188.672 770.986667 155.008 755.2 155.008 755.2c-46.378667-31.744 3.584-31.104 3.584-31.104 51.413333 3.584 78.421333 52.736 78.421333 52.736 45.653333 78.293333 119.850667 55.68 149.12 42.581333 4.608-33.109333 17.792-55.68 32.426667-68.48-113.706667-12.8-233.216-56.832-233.216-253.013333.0-55.893333 19.84-101.546667 52.693333-137.386667-5.76-12.928-23.04-64.981333 4.48-135.509333.0.0 42.88-13.738667 140.8 52.48 40.96-11.392 84.48-17.024 128-17.28 43.52.256 87.04 5.888 128 17.28 97.28-66.218667 140.16-52.48 140.16-52.48 27.52 70.528 10.24 122.581333 5.12 135.509333 32.64 35.84 52.48 81.493333 52.48 137.386667.0 196.693333-119.68 240-233.6 252.586667 17.92 15.36 34.56 46.762667 34.56 94.72.0 68.522667-.64 123.562667-.64 140.202666.0 13.44 8.96 29.44 35.2 24.32C877.44 942.592 1024 750.592 1024 524.672c0-282.752-229.248-512-512-512"/></svg></a><a href=https://owulveryck.github.io/index.xml rel="noopener alternate" type=application/rss&#43;xml class=iconfont title=rss target=_blank><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="30" height="30"><path d="M819.157333 1024C819.157333 574.592 449.408 204.8.0 204.8V0c561.706667.0 1024 462.293333 1024 1024H819.157333zM140.416 743.04a140.8 140.8.0 0 1 140.501333 140.586667A140.928 140.928.0 0 1 140.074667 1024C62.72 1024 0 961.109333.0 883.626667S62.933333 743.082667 140.416 743.04zM678.784 1024h-199.04c0-263.210667-216.533333-479.786667-479.744-479.786667V345.173333c372.352.0 678.784 306.517333 678.784 678.826667z"/></svg></a></div><div class=copyright><span class=power-by>Powered by <a class=hexo-link href=https://gohugo.io>Hugo</a></span>
<span class=division>|</span>
<span class=theme-info>Theme - <a class=theme-link href=https://github.com/xianmin/hugo-theme-jane>Jane</a></span>
<span class=copyright-year>&copy;
2015 -
2020
<span class=heart><i class=iconfont><svg class="icon" viewBox="0 0 1025 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="14" height="14"><path d="M1000.1 247.9c-15.5-37.3-37.6-70.6-65.7-98.9-54.4-54.8-125.8-85-201-85-85.7.0-166 39-221.4 107.4C456.6 103 376.3 64 290.6 64c-75.1.0-146.5 30.4-201.1 85.6-28.2 28.5-50.4 61.9-65.8 99.3-16 38.8-24 79.9-23.6 122.2.7 91.7 40.1 177.2 108.1 234.8 3.1 2.6 6 5.1 8.9 7.8 14.9 13.4 58 52.8 112.6 102.7 93.5 85.5 209.9 191.9 257.5 234.2 7 6.1 15.8 9.5 24.9 9.5 9.2.0 18.1-3.4 24.9-9.5 34.5-30.7 105.8-95.9 181.4-165 74.2-67.8 150.9-138 195.8-178.2 69.5-57.9 109.6-144.4 109.9-237.3.1-42.5-8-83.6-24-122.2z" fill="#8a8a8a"/></svg></i></span><span class=author>Olivier Wulveryck</span></span></div></footer><div class=back-to-top id=back-to-top><i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="35" height="35"><path d="M510.866688 227.694839 95.449397 629.218702h235.761562L329.15309 958.01517h362.40389L691.55698 628.188232l241.942331-3.089361L510.866688 227.694839zM63.840492 63.962777h894.052392v131.813095H63.840492V63.962777v0zm0 0"/></svg></i></div></div><script type=text/javascript src=/lib/jquery/jquery-3.2.1.min.js></script><script type=text/javascript src=/lib/slideout/slideout-1.0.1.min.js></script><script type=text/javascript src=/js/main.638251f4230630f0335d8c6748e53a96f94b72670920b60c09a56fdc8bece214.js integrity="sha256-Y4JR9CMGMPAzXYxnSOU6lvlLcmcJILYMCaVv3Ivs4hQ=" crossorigin=anonymous></script><script type=text/javascript src=/js/load-photoswipe.js></script><script type=text/javascript src=/lib/photoswipe/photoswipe.min.js></script><script type=text/javascript src=/lib/photoswipe/photoswipe-ui-default.min.js></script></body></html>