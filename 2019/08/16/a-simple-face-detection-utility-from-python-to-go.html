<!doctype html><html lang=en itemscope itemtype=http://schema.org/WebPage><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><title>A simple face detection utility from Python to Go - Unladen swallow - Olivier Wulveryck</title><meta name=renderer content=webkit><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta name=MobileOptimized content=width><meta name=HandheldFriendly content=true><meta name=applicable-device content=pc,mobile><meta name=theme-color content=#f8f5ec><meta name=msapplication-navbutton-color content=#f8f5ec><meta name=apple-mobile-web-app-capable content=yes><meta name=apple-mobile-web-app-status-bar-style content=#f8f5ec><meta name=mobile-web-app-capable content=yes><meta name=author content="Olivier Wulveryck"><meta name=description content="This post describes how to build a face detection tool with a neural network. The full conception is described, from the design to the implementation."><meta name=keywords content="onnx,DDD,Keras,Go,Neural Net,YOLO"><meta name=generator content="Hugo 0.58.3"><link rel=canonical href=https://owulveryck.github.io/2019/08/16/a-simple-face-detection-utility-from-python-to-go.html><link rel=icon href=/favicon.ico><link rel=stylesheet href=/sass/jane.min.b3a8813c06e6d785beba22bf8264e174fa2cb3a396b22f9ba24e2c00c18aaf7f.css integrity="sha256-s6iBPAbm14W&#43;uiK/gmThdPoss6OWsi&#43;bok4sAMGKr38=" media=screen crossorigin=anonymous><meta property=og:title content="A simple face detection utility from Python to Go"><meta property=og:description content="This post describes how to build a face detection tool with a neural network. The full conception is described, from the design to the implementation."><meta property=og:type content=article><meta property=og:url content=https://owulveryck.github.io/2019/08/16/a-simple-face-detection-utility-from-python-to-go.html><meta property=article:published_time content=2019-08-16T21:25:30+02:00><meta property=article:modified_time content=2019-08-16T21:25:30+02:00><meta itemprop=name content="A simple face detection utility from Python to Go"><meta itemprop=description content="This post describes how to build a face detection tool with a neural network. The full conception is described, from the design to the implementation."><meta itemprop=datePublished content=2019-08-16T21:25:30&#43;02:00><meta itemprop=dateModified content=2019-08-16T21:25:30&#43;02:00><meta itemprop=wordCount content=2224><meta itemprop=keywords content="YOLO,computer vision,onnx,gorgonia,golang,AI,"><meta name=twitter:card content=summary><meta name=twitter:title content="A simple face detection utility from Python to Go"><meta name=twitter:description content="This post describes how to build a face detection tool with a neural network. The full conception is described, from the design to the implementation."><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script><script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]--><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-69673850-1','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script></head><body><div id=mobile-navbar class=mobile-navbar><div class=mobile-header-logo><a href=/ class=logo>owulveryck's blog</a></div><div class=mobile-navbar-icon><span></span><span></span><span></span></div></div><nav id=mobile-menu class="mobile-menu slideout-menu"><ul class=mobile-menu-list><li class=mobile-menu-item><a class=menu-item-link href=https://owulveryck.github.io/>This is Home</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://owulveryck.github.io/post/>Archives</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://owulveryck.github.io/tags/>Tags</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://owulveryck.github.io/categories/>Categories</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://owulveryck.github.io/about.html>About</a></li></ul></nav><link rel=stylesheet href=/lib/photoswipe/photoswipe.min.css><link rel=stylesheet href=/lib/photoswipe/default-skin/default-skin.min.css><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><header id=header class="header container"><div class=logo-wrapper><a href=/ class=logo>owulveryck's blog</a></div><nav class=site-navbar><ul id=menu class=menu><li class=menu-item><a class=menu-item-link href=https://owulveryck.github.io/>This is Home</a></li><li class=menu-item><a class=menu-item-link href=https://owulveryck.github.io/post/>Archives</a></li><li class=menu-item><a class=menu-item-link href=https://owulveryck.github.io/tags/>Tags</a></li><li class=menu-item><a class=menu-item-link href=https://owulveryck.github.io/categories/>Categories</a></li><li class=menu-item><a class=menu-item-link href=https://owulveryck.github.io/about.html>About</a></li></ul></nav></header><div id=mobile-panel><main id=main class="main bg-llight"><div class=content-wrapper><div id=content class="content container"><article class="post bg-white"><header class=post-header><h1 class=post-title>A simple face detection utility from Python to Go</h1><div class=post-meta><time datetime=2019-08-16 class=post-time>2019-08-16</time></div></header><div class=post-toc id=post-toc><h2 class=post-toc-title>Table of Contents</h2><div class=post-toc-content><nav id=TableOfContents><ul><li><ul><li><ul><li><a href=#overall-picture>Overall picture</a></li></ul></li></ul></li><li><a href=#implementing-the-business-logic-with-a-neural-network>Implementing the business logic with a neural network</a><ul><li><ul><li><a href=#getting-the-weights>Getting the weights</a></li><li><a href=#combining-the-weights-and-the-model>Combining the weights and the model</a></li><li><a href=#generate-the-onnx-file>Generate the onnx file</a><ul><li><a href=#model-visualization>Model visualization</a></li><li><a href=#preparing-the-test-of-the-infrastructure>Preparing the test of the infrastructure</a></li></ul></li></ul></li></ul></li><li><a href=#infrastructure-entering-the-go-world>Infrastructure: Entering the Go world</a><ul><li><ul><li><a href=#the-service-provider-interface-spi>The Service Provider Interface (SPI)</a><ul><li><a href=#testing-the-infrastructure>Testing the infrastructure</a></li></ul></li></ul></li></ul></li><li><a href=#writing-the-application-in-go>Writing the application in Go</a><ul><li><a href=#the-api>The API</a><ul><li><a href=#input>Input</a><ul><li><a href=#gettensorfromimage>GetTensorFromImage</a></li></ul></li><li><a href=#output>Output</a><ul><li><a href=#bounding-boxes>Bounding boxes</a></li><li><a href=#get-the-bounding-boxes>Get the bounding boxes</a></li></ul></li></ul></li></ul></li><li><a href=#final-result>Final result</a><ul><li><a href=#example>Example</a></li><li><a href=#going-a-bit-further-getting-an-output-picture>Going a bit further: getting an output picture</a></li></ul></li><li><a href=#conclusion>Conclusion</a></li></ul></nav></div></div><div class=post-content><p>In this article, I explain how to build a tool to detect faces in a picture.
This article is a sort of how-to design and implements a tool by using a neural network.</p><p>For the design part, I describe how to:</p><ul><li>build the business model thanks to a neural network;</li><li>adapt the network to the specific domain of face detection by changing its knowledge;</li><li>use the resulting domain with a go-based infrastructure;</li><li>code a little application in Go to communicate with the outside world.</li></ul><p>On the technical side, I am using the following technologies:</p><ul><li>Python / Keras</li><li>ONNX</li><li>Go</li></ul><p><strong>Note</strong>: Some of the terms such as <em>domain</em>, <em>application</em>, and <em>infrastructure</em> refer to the concepts from Domain Driver Design (DDD) or the hexagonal architecture. For example, do not consider the infrastructure as boxes and wires, but see it as a service layer. The infrastructure represents everything that exists independently of the application.</p><p><strong>Disclaimer</strong>: I am using those concepts to illustrate what I do; This is not a proper DDD design nor an authentic hexagonal architecture.</p><h3 id=overall-picture>Overall picture</h3><p>Those layers can represent the architecture of the tool:
<center><figure><img src=/assets/yolofaces/archi1.png><figcaption><h4>An overall picture of the architecture</h4></figcaption></figure></center></p><p>The basic principle is that every layer is a &ldquo;closed area&rdquo;; therefore, it is accessible through API, and every layer is testable independently.
Different paragraphs of this post describe each layer.</p><p>The &ldquo;actor&rdquo; here is a simple CLI tool. It is the main package of the application (and in go the main package is the package <code>main</code>); In the rest of the article, I reference it as &ldquo;<strong>the actor</strong>&rdquo;.</p><h1 id=implementing-the-business-logic-with-a-neural-network>Implementing the business logic with a neural network</h1><p>The core functionality of the tool is to detect faces on a picture.
I am using a neural network to achieve this. The model I have chosen is
<a href=https://pjreddie.com/darknet/yolov2/>Tiny YOLO v2</a>, which can perform real-time object detection.</p><blockquote><p>This model is designed to be small but powerful. It attains the same top-1 and top-5 performance as AlexNet but with 1/10th the parameters. It uses mostly convolutional layers without the large fully connected layers at the end. It is about twice as fast as AlexNet on CPU making it more suitable for some vision applications.</p></blockquote><p>I am using the &ldquo;tiny&rdquo; version, which is based on the Darknet reference network and is much faster but less accurate than the regular YOLO model.</p><p>The model is just an “envelope.” It needs some training to be able to detect some objects. The objects it can detect is dependant of its knowledge. The weights tensors represent its knowledge.
To detect faces, we need to apply the model to the picture with a knowledge (some weights) able to recognize faces.</p><blockquote><p>The model is the envelope; it can detect many objects. The knowledge that makes it able to detect faces is in the weights.</p></blockquote><h3 id=getting-the-weights>Getting the weights</h3><p>By luck, an engineer named <a href=https://github.com/azmathmoosa>Azmath Moosa</a> has trained the model and released a tool called <a href=https://github.com/azmathmoosa/azFace>azface</a>.
The project is available on GitHub in LGPLv3 but, it does not contain the sources of the tool (only a Windows binary and some DLL are present). However, what I am interested in is not the tool as I am building my own. What I am seeking now is the weights, and the weights are present in the repository as well.</p><p><em>Disclaimer</em>: the tool we are building is for academic purpose. I am not competing with Azmath&rsquo;s tool in any way.</p><p>First, we clone the repository to have the weights locally:</p><p><code>$ git clone https://github.com/azmathmoosa/azFace</code></p><p>The weights are this heavy file of 61Mb: <code>weights/tiny-yolo-azface-fddb_82000.weights</code>.</p><h3 id=combining-the-weights-and-the-model>Combining the weights and the model</h3><p>Now, we need to combine the knowledge and the model. Together, they constitute the core functionality of our domain.</p><p>The business logic should be as independent as possible of any framework. The best way to represent the neural network is to be as close as possible as
its definition; The original implementation of the YOLO model (from &ldquo;darknet&rdquo;) is in C; There are other reimplementations in Tensorflow, Keras, Java, &hellip;</p><p>I am using <a href=https://onnx.ai/>ONNX</a> as a format for the business logic; It is an Intermediate Representation that is, as a consequence, independant of a framework.</p><p>To create the ONNX format, I am using Keras with thei following tools:</p><ul><li><a href=https://github.com/allanzelener/yad2k.git><code>yad2k</code></a> to create a Keras model from YOLO;</li><li><a href=https://pypi.org/project/keras2onnx/><code>keras2onnx</code></a> to encode it into ONNX.</li></ul><p>The workflow is:</p><pre><code>                          yad2k                   keras2onnx               
darknet config + weights --------&gt;  keras model --------------&gt; onnx model
</code></pre><p>This script creates a Keras model from the config and the weights of <code>azface</code></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code class=language-bash data-lang=bash><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-bash data-lang=bash>./yad2k.py <span class=se>\
</span><span class=se></span>        ../azFace/net_cfg/tiny-yolo-azface-fddb.cfg <span class=se>\
</span><span class=se></span>        ../azFace/weights/tiny-yolo-azface-fddb_82000.weights <span class=se>\
</span><span class=se></span>        ../FACES/keras/yolo2.h5</code></pre></td></tr></table></div></div><p>It generates a pre-trained <a href=https://drive.google.com/file/d/1O4BF8m3WrrHTIHnqFtl2oghaw_esRaYn/view>h5 version</a> of the tiny YOLO v2 model, able to find faces.</p><p>Then, analyzing the resulting model with this code snippet gives the following result:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code class=language-python data-lang=python><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-python data-lang=python><span class=kn>from</span> <span class=nn>keras.models</span> <span class=kn>import</span> <span class=n>load_model</span>
<span class=n>keras_model</span><span class=o>=</span> <span class=n>load_model</span><span class=p>(</span><span class=s1>&#39;../FACES/keras/yolo.h5&#39;</span><span class=p>)</span>
<span class=n>keras_model</span><span class=o>.</span><span class=n>summary</span><span class=p>()</span></code></pre></td></tr></table></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code class=language-txt data-lang=txt><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-txt data-lang=txt>_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 416, 416, 3)       0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 416, 416, 16)      432       
_________________________________________________________________
...
_________________________________________________________________
conv2d_9 (Conv2D)            (None, 13, 13, 30)        30750     
=================================================================
Total params: 15,770,510
Trainable params: 15,764,398
Non-trainable params: 6,112
_________________________________________________________________</code></pre></td></tr></table></div></div><p>The resulting model looks ok.</p><h3 id=generate-the-onnx-file>Generate the onnx file</h3><p>To generate the ONNX representation of the model, I use <a href=https://github.com/onnx/keras-onnx>keras2onnx</a>:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code class=language-python data-lang=python><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-python data-lang=python><span class=kn>import</span> <span class=nn>onnxmltools</span>
<span class=kn>import</span> <span class=nn>onnx</span>
<span class=kn>import</span> <span class=nn>keras2onnx</span>
<span class=kn>from</span> <span class=nn>keras.models</span> <span class=kn>import</span> <span class=n>load_model</span>

<span class=n>keras_model</span><span class=o>=</span> <span class=n>load_model</span><span class=p>(</span><span class=s1>&#39;../FACES/keras/yolo.h5&#39;</span><span class=p>)</span>
<span class=n>onnx_model</span> <span class=o>=</span> <span class=n>keras2onnx</span><span class=o>.</span><span class=n>convert_keras</span><span class=p>(</span><span class=n>keras_model</span><span class=p>,</span> <span class=n>name</span><span class=o>=</span><span class=bp>None</span><span class=p>,</span> <span class=n>doc_string</span><span class=o>=</span><span class=s1>&#39;&#39;</span><span class=p>,</span> <span class=n>target_opset</span><span class=o>=</span><span class=bp>None</span><span class=p>,</span> <span class=n>channel_first_inputs</span><span class=o>=</span><span class=bp>None</span><span class=p>)</span>
<span class=n>onnx</span><span class=o>.</span><span class=n>save</span><span class=p>(</span><span class=n>onnx_model</span><span class=p>,</span> <span class=s1>&#39;../FACES/yolo.onnx&#39;</span><span class=p>)</span></code></pre></td></tr></table></div></div><h4 id=model-visualization>Model visualization</h4><p>It is interesting to visualize the result of the conversion. I am using the tool <code>netron</code> which have a <a href=https://lutzroeder.github.io/netron/>web version</a>.</p><p>Here is an extract of the picture it generates:
<center><figure><img src=/assets/yolofaces/netron-extract.png link=/assets/yolofaces/netron.png width=50%><figcaption><h4>Netron representation of the tiny YOLO v2 graph</h4></figcaption></figure></center></p><p>I made a copy of the full representation <a href=/assets/yolofaces/netron.png>here</a> if you want to see how the model looks.</p><h4 id=preparing-the-test-of-the-infrastructure>Preparing the test of the infrastructure</h4><p>To validate our future infrastructure, I need a simple test.</p><p>What I am doing is applying the model on a zero value and save the result. I will do the same once the final infrastructure is up and compare the results.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code class=language-python data-lang=python><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-python data-lang=python><span class=kn>from</span> <span class=nn>keras.models</span> <span class=kn>import</span> <span class=n>load_model</span>
<span class=kn>import</span> <span class=nn>numpy</span> <span class=kn>as</span> <span class=nn>np</span>
<span class=n>keras_model</span><span class=o>=</span> <span class=n>load_model</span><span class=p>(</span><span class=s1>&#39;../FACES/keras/yolo.h5&#39;</span><span class=p>)</span>

<span class=n>output</span> <span class=o>=</span> <span class=n>keras_model</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>zeros</span><span class=p>((</span><span class=mi>1</span><span class=p>,</span><span class=mi>416</span><span class=p>,</span><span class=mi>416</span><span class=p>,</span><span class=mi>3</span><span class=p>)))</span>
<span class=n>np</span><span class=o>.</span><span class=n>save</span><span class=p>(</span><span class=s2>&#34;../FACES/keras/output.npy&#34;</span><span class=p>,</span><span class=n>output</span><span class=p>)</span></code></pre></td></tr></table></div></div><p>Now, let&rsquo;s move to the infrastructure and application part.</p><h1 id=infrastructure-entering-the-go-world>Infrastructure: Entering the Go world</h1><p>No surprises here: the infrastructure I am using is made of <a href=https://github.com/owulveryck/onnx-go><code>onnx-go</code></a> to decode the onnx file,
and <a href=https://github.com/gorgonia/gorgonia>Gorgonia</a> to execute the model.
This solution is an efficient solution for a tool; at runtime, it does not need any of the dependencies used to build the network (no more <em>Python</em>, <em>Tensorflow</em>, <em>Conda</em>, etc.). It gives the end-user of the tool a much better experience.</p><h3 id=the-service-provider-interface-spi>The Service Provider Interface (SPI)</h3><p>We&rsquo;ve seen its model represents the neural network. The SPI should implement a model to fulfill the contract and understand the ONNX Intermediate Representation (IR). <a href=https://github.com/owulveryck/onnx-go>Onnx-go</a>&rsquo;s <a href=https://godoc.org/github.com/owulveryck/onnx-go#Model><code>Model</code></a> object is a Go structure that acts as a receiver of the neural network model.</p><p>The other service required is a computation engine that understands and executes the model. <a href=https://github.com/gorgonia/gorgonia>Gorgonia</a> assumes this function.</p><p>The <strong>actor</strong> uses those services. A basic implementation in Go is (note the package is <code>main</code>):</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code class=language-go data-lang=go><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-go data-lang=go><span class=kn>import</span> <span class=p>(</span>
        <span class=s>&#34;github.com/owulveryck/onnx-go&#34;</span>
        <span class=s>&#34;github.com/owulveryck/onnx-go/backend/x/gorgonnx&#34;</span>
<span class=p>)</span>

<span class=kd>func</span> <span class=nf>main</span><span class=p>()</span> <span class=p>{</span>
        <span class=nx>b</span><span class=p>,</span> <span class=nx>_</span> <span class=o>:=</span> <span class=nx>ioutil</span><span class=p>.</span><span class=nf>ReadFile</span><span class=p>(</span><span class=s>&#34;../FACES/yolo.onnx&#34;</span><span class=p>)</span>
        <span class=nx>backend</span> <span class=o>:=</span> <span class=nx>gorgonnx</span><span class=p>.</span><span class=nf>NewGraph</span><span class=p>()</span>
        <span class=nx>model</span> <span class=o>:=</span> <span class=nx>onnx</span><span class=p>.</span><span class=nf>NewModel</span><span class=p>(</span><span class=nx>backend</span><span class=p>)</span>
        <span class=nx>model</span><span class=p>.</span><span class=nf>UnmarshalBinary</span><span class=p>(</span><span class=nx>b</span><span class=p>)</span>
<span class=p>}</span></code></pre></td></tr></table></div></div><p>To use the model, we need to interact with its inputs and output.
The model takes a tensor as input. To set this input, the <code>onnx-go</code> library provides a helper function called <a href=https://godoc.org/github.com/owulveryck/onnx-go#Model.SetInput><code>SetInput</code></a>.</p><p>For the output, a call to <a href=https://godoc.org/github.com/owulveryck/onnx-go#Model.GetOutputTensors><code>GetOutputTensors()</code></a> extracts the resulting tensors.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code class=language-go data-lang=go><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-go data-lang=go><span class=nx>t</span> <span class=o>:=</span> <span class=nx>tensor</span><span class=p>.</span><span class=nf>New</span><span class=p>(</span>
        <span class=nx>tensor</span><span class=p>.</span><span class=nf>WithShape</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>416</span><span class=p>,</span> <span class=mi>416</span><span class=p>,</span> <span class=mi>3</span><span class=p>),</span> 
        <span class=nx>tensor</span><span class=p>.</span><span class=nf>Of</span><span class=p>(</span><span class=nx>tensor</span><span class=p>.</span><span class=nx>Float32</span><span class=p>))</span>
<span class=nx>model</span><span class=p>.</span><span class=nf>SetInput</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=nx>t</span><span class=p>)</span></code></pre></td></tr></table></div></div><p>The <strong>actor</strong> can use those methods, but, as the goal of the application is to analyze pictures, the application is going to encapsulate them. It provides a better user experience for the actor (the actors will probably not want to mess up with tensors).</p><h4 id=testing-the-infrastructure>Testing the infrastructure</h4><p>We can now test the infrastructure to see if the implementation is ok. We set an empty tensor, compute it with Gorgonia, and compare the result with the one
saved previously:</p><p>I wrote a small <code>test</code> file in the go format; for clarity, I am not copying it here, but you can find it in this <a href=https://gist.github.com/owulveryck/3d15c0eb9cf7dea6518116ec0a5be581#file-yolo_test-go>gist</a>.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code class=language-text data-lang=text><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-text data-lang=text># go test
PASS
ok      tmp/graph       1.054s</code></pre></td></tr></table></div></div><p><em>Note</em>: The ExprGraph used by Gorgonia can also be represented visually with Graphviz. This code generates the <em>dot</em> representation:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code class=language-go data-lang=go><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-go data-lang=go><span class=nx>exprGraph</span><span class=p>,</span> <span class=nx>_</span> <span class=o>:=</span> <span class=nx>backend</span><span class=p>.</span><span class=nf>GetExprGraph</span><span class=p>()</span>
<span class=nx>b</span><span class=p>,</span> <span class=nx>_</span> <span class=o>:=</span> <span class=nx>dot</span><span class=p>.</span><span class=nf>Marshal</span><span class=p>(</span><span class=nx>exprGraph</span><span class=p>)</span>
<span class=nx>fmt</span><span class=p>.</span><span class=nf>Println</span><span class=p>(</span><span class=nb>string</span><span class=p>(</span><span class=nx>b</span><span class=p>))</span>
<span class=p>}</span></code></pre></td></tr></table></div></div><p>(the full graph is <a href=/assets/yolofaces/yolo-gorgonia.png>here</a>)</p><p><center><figure><img src=/assets/yolofaces/onnx-gorgonia-preview.png width=50%><figcaption><h4>Gorgonia representation of the tiny YOLO v2 graph</h4></figcaption></figure></center></p><p>The infrastructure is ok, and is implementing the SPI! Let&rsquo;s move to the application part!</p><h1 id=writing-the-application-in-go>Writing the application in Go</h1><h2 id=the-api>The API</h2><p>Let&rsquo;s start with the interface of the application. I create a package <code>gofaces</code> to hold the logic of the application.
It is a layer that adds some facilities to communicate with the outside world. This package is instantiable by anything from a simple CLI to
a web service.</p><h3 id=input>Input</h3><h4 id=gettensorfromimage>GetTensorFromImage</h4><p>This function takes an image as input; The image is transferred to the function with a stream of bytes (<code>io.Reader</code>). It let the possibility for the end-user
to use a regular file, to get the content from stdin, or to build a web service and get the file via HTTP.
This function returns a tensor usable with the model; it also returns an error if it cannot process the file.</p><p><em>Note</em> the full signature of the <code>GetTensorFromImage</code> function can be found on <a href=https://godoc.org/github.com/owulveryck/gofaces#GetTensorFromImage>GoDoc</a></p><p>If we switch back to <strong>actor</strong> implementation, we can now set an input picture with this code: (I skip the errors checking for clarity):</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code class=language-go data-lang=go><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-go data-lang=go><span class=kd>func</span> <span class=nf>main</span><span class=p>()</span> <span class=p>{</span>
        <span class=nx>b</span><span class=p>,</span> <span class=nx>_</span> <span class=o>:=</span> <span class=nx>ioutil</span><span class=p>.</span><span class=nf>ReadFile</span><span class=p>(</span><span class=s>&#34;../FACES/yolo.onnx&#34;</span><span class=p>)</span>
        <span class=c1>// Instanciate the infrastructure
</span><span class=c1></span>        <span class=nx>backend</span> <span class=o>:=</span> <span class=nx>gorgonnx</span><span class=p>.</span><span class=nf>NewGraph</span><span class=p>()</span>
        <span class=nx>model</span> <span class=o>:=</span> <span class=nx>onnx</span><span class=p>.</span><span class=nf>NewModel</span><span class=p>(</span><span class=nx>backend</span><span class=p>)</span>
        <span class=c1>// Loading the business logic (the neural net)
</span><span class=c1></span>        <span class=nx>model</span><span class=p>.</span><span class=nf>UnmarshalBinary</span><span class=p>(</span><span class=nx>b</span><span class=p>)</span>
        <span class=c1>// Accessing the I/O through the API
</span><span class=c1></span>        <span class=nx>inputT</span><span class=p>,</span> <span class=nx>_</span> <span class=o>:=</span> <span class=nx>gofaces</span><span class=p>.</span><span class=nf>GetTensorFromImage</span><span class=p>(</span><span class=nx>img</span><span class=p>)</span>
        <span class=nx>model</span><span class=p>.</span><span class=nf>SetInput</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=nx>inputT</span><span class=p>)</span>
<span class=p>}</span></code></pre></td></tr></table></div></div><p>To run the model, we call the function <a href="Gorgonia fulfills the [`ComputationBackend`](https://godoc.org/github.com/owulveryck/onnx-go/backend#ComputationBackend) interface"><code>backend.Run()</code></a>.</p><h3 id=output>Output</h3><h4 id=bounding-boxes>Bounding boxes</h4><p>The model outputs a tensor. This tensor holds all pieces of information required to extract bounding boxes.
Getting the bounding boxes is the responsibility of the application. Therefore, the package <code>gofaces</code> defines a <a href=https://godoc.org/github.com/owulveryck/gofaces#Box><code>Box</code></a> structure.<br>A box contains a set of <a href=https://godoc.org/github.com/owulveryck/gofaces#Element><code>Elements</code></a></p><h4 id=get-the-bounding-boxes>Get the bounding boxes</h4><p>The application&rsquo;s goal is to analyze the picture and to provide the bounding boxes that contain faces.
What the <strong>actor</strong> needs are the resulting bounding boxes.
The application provides them via a call to the <a href=https://godoc.org/github.com/owulveryck/gofaces#ProcessOutput><code>ProcessOutput</code></a> function.</p><p><em>Note</em> On top of this function, I include a function to <a href=https://godoc.org/github.com/owulveryck/gofaces#Sanitize><code>Sanitize</code></a> the results (which could be in a separate package though because it is part of the post-processing).</p><h1 id=final-result>Final result</h1><p>You can find the code of the application in my <a href=https://github.com/owulveryck/gofaces><code>gofaces</code></a> repository.</p><p>The repository is composed of:</p><ul><li>the <code>gofaces</code> package which is at the root level (see the godoc <a href=https://godoc.org/github.com/owulveryck/gofaces>here</a>;</li><li>a <code>cmd</code> subdirectory is holding a sample implementation to analyze the picture in the command line.</li></ul><h2 id=example>Example</h2><p>I am using a famous meme as input.
<center><figure><img src=/assets/yolofaces/meme.jpg width=30%></figure></center></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell><span class=nb>cd</span> <span class=nv>$GOPATH</span>/src/github.com/owulveryck/gofaces/cmd
go run main.go <span class=se>\
</span><span class=se></span>        -img /tmp/meme.jpg <span class=se>\
</span><span class=se></span>        -model ../model/model.onnx</code></pre></td></tr></table></div></div><p>gives the following result</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code class=language-text data-lang=text><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-text data-lang=text>[At (187,85)-(251,147) (confidence 0.20):
        - face - 1
]</code></pre></td></tr></table></div></div><p>It has detected only one face; It is possible to play with the confidence threshold to detect other faces.
I have found that it is not possible to detect the face of the <em>lover</em>; probably because the picture does not show her full face.</p><h2 id=going-a-bit-further-getting-an-output-picture>Going a bit further: getting an output picture</h2><p>It is not the responsibility of the <code>gofaces</code> package to generate a picture; its goal is to detect faces only.
I have included in the repository another package, <a href=https://godoc.org/github.com/owulveryck/gofaces/draw><code>draw</code></a>. This package contains a single exported function.
This function generates a Go <code>image.Image</code> with a transparent background and add the rectangles of the boxes.</p><p>I tweaked the primary tool to add an <code>-output</code> flag (in the <code>main</code> package). It writes a png file you can combine it with the original picture in post-processing.</p><p>Here is an example of post processing with <a href=https://imagemagick.org/index.php>ImageMagick</a>.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span><span class=lnt>9
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell><span class=nv>YOLO_CONFIDENCE_THRESHOLD</span><span class=o>=</span><span class=m>0</span>.1 go run main.go <span class=se>\
</span><span class=se></span>        -img /tmp/meme.jpg <span class=se>\
</span><span class=se></span>        -output /tmp/mask2.png <span class=se>\
</span><span class=se></span>        -model ../model/model.onnx
convert <span class=se>\
</span><span class=se></span>        /tmp/meme.jpg <span class=se>\
</span><span class=se></span>        /tmp/mask2.png <span class=se>\
</span><span class=se></span>        <span class=se>\(</span> -resize 418x <span class=se>\)</span> <span class=se>\
</span><span class=se></span>        -compose over -composite /tmp/result2.png</code></pre></td></tr></table></div></div><p><center><img src=/assets/yolofaces/mask2.png width=30% style=border-width:1px;border-color:#000;border-style:solid>
<img src=/assets/yolofaces/result2.png width=30%></center></p><h1 id=conclusion>Conclusion</h1><p>Alongside this article, we made a tool by writing three testable packages (<code>gofaces</code>, <code>draw</code> and, obviously, <code>main</code>).</p><p>The Go self-contained binary makes it the right choice for playing with face detection on personal computers. On top of that, It is easy, for a developer, to adapt the tool by tweaking only the <code>main</code> package. He can use face detection to write the funniest or fanciest tool. The sky is the limit.</p><p>Thanks to the ONNX Intermediate Representation (IR), it is now possible to use machine learning to describe part of the business logic of a tool.
Third-party implementations of the ONNX format allows writing efficient applications with different frameworks or runtime environments.</p><p>What I like the most with this idea is that we have a separation of concerns for building a modular and testable tool.
Each part can have its lifecycle as long as they still fulfill the interfaces.</p><p>On top of that, each layer is fully testable, which brings quality in the final result.</p></div><footer class=post-footer><div class=post-tags><a href=https://owulveryck.github.io/tags/yolo.html>YOLO</a>
<a href=https://owulveryck.github.io/tags/computer-vision.html>computer vision</a>
<a href=https://owulveryck.github.io/tags/onnx.html>onnx</a>
<a href=https://owulveryck.github.io/tags/gorgonia.html>gorgonia</a>
<a href=https://owulveryck.github.io/tags/golang.html>golang</a>
<a href=https://owulveryck.github.io/tags/ai.html>AI</a></div><nav class=post-nav><a class=prev href=/2019/10/14/think-like-a-vertex-using-gos-concurrency-for-graph-computation.html><i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="18" height="18"><path d="M691.908486 949.511495l75.369571-89.491197c10.963703-12.998035 10.285251-32.864502-1.499144-44.378743L479.499795 515.267417l277.93508-310.326815c11.338233-12.190647 11.035334-32.285311-.638543-44.850487l-80.46666-86.564541c-11.680017-12.583596-30.356378-12.893658-41.662889-.716314L257.233596 494.235404c-11.332093 12.183484-11.041474 32.266891.657986 44.844348l80.46666 86.564541c1.772366 1.910513 3.706415 3.533476 5.750981 4.877077l306.620399 321.703933C662.505829 963.726242 680.945807 962.528973 691.908486 949.511495z"/></svg></i><span class="prev-text nav-default">Think like a vertex: using Go&#39;s concurrency for graph computation</span>
<span class="prev-text nav-mobile">Prev</span></a>
<a class=next href=/2019/04/03/from-a-project-to-a-product-the-state-of-onnx-go.html><span class="next-text nav-default">From a project to a product: the state of onnx-go</span>
<span class="prev-text nav-mobile">Next</span>
<i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="18" height="18"><path d="M332.091514 74.487481l-75.369571 89.491197c-10.963703 12.998035-10.285251 32.864502 1.499144 44.378743l286.278095 300.375162L266.565125 819.058374c-11.338233 12.190647-11.035334 32.285311.638543 44.850487l80.46666 86.564541c11.680017 12.583596 30.356378 12.893658 41.662889.716314l377.434212-421.426145c11.332093-12.183484 11.041474-32.266891-.657986-44.844348l-80.46666-86.564541c-1.772366-1.910513-3.706415-3.533476-5.750981-4.877077L373.270379 71.774697C361.493148 60.273758 343.054193 61.470003 332.091514 74.487481z"/></svg></i></a></nav></footer></article><div class=disqus-comment><div class=disqus-button id=load_disqus onclick=load_disqus()>Show Disqus Comments</div><div id=disqus_thread></div><script type=text/javascript>var disqus_config=function(){this.page.url="https://owulveryck.github.io/2019/08/16/a-simple-face-detection-utility-from-python-to-go.html";};function load_disqus(){if(window.location.hostname==='localhost')return;var dsq=document.createElement('script');dsq.type='text/javascript';dsq.async=true;var disqus_shortname='owulveryck';dsq.src='//'+disqus_shortname+'.disqus.com/embed.js';(document.getElementsByTagName('head')[0]||document.getElementsByTagName('body')[0]).appendChild(dsq);$('#load_disqus').remove();};</script><noscript>Please enable JavaScript to view the
<a href=http://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript></div></div></div></main><footer id=footer class=footer><div class=icon-links><a href=https://twitter.com/owulveryck rel="me noopener" class=iconfont title=twitter target=_blank><svg class="icon" viewBox="0 0 1264 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="36" height="36"><path d="M1229.8616 18.043658c0 0-117.852626 63.135335-164.151872 67.344358-105.225559-164.151872-505.082682-92.598492-437.738325 223.078185C278.622548 312.675223 89.216542 47.506814 89.216542 47.506814s-117.852626 189.406006 75.762402 345.139833C127.097743 396.85567 55.544363 371.601535 55.544363 371.601535S26.081207 535.753407 253.368414 615.724832c-21.045112 29.463156-113.643603 8.418045-113.643603 8.418045s25.254134 143.10676 231.496229 180.987961c-143.10676 130.479693-387.230056 92.598492-370.393967 105.225559 206.242095 189.406006 1119.599946 231.496229 1128.01799-643.98042C1179.353331 249.539887 1263.533778 123.269217 1263.533778 123.269217s-130.479693 37.881201-138.897738 33.672179C1225.652577 98.015083 1229.8616 18.043658 1229.8616 18.043658"/></svg></a><a href=https://www.linkedin.com/in/olivierwulveryck/ rel="me noopener" class=iconfont title=linkedin target=_blank><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="33" height="33"><path d="M872.405333 872.618667H720.768v-237.610667c0-56.661333-1.152-129.578667-79.018667-129.578667-79.061333.0-91.136 61.653333-91.136 125.397334v241.792H398.976V384H544.64v66.602667h1.962667c20.352-38.4 69.845333-78.933333 143.786666-78.933334 153.642667.0 182.058667 101.12 182.058667 232.746667v268.202667zM227.712 317.141333a87.978667 87.978667.0 0 1-88.021333-88.106666A88.064 88.064.0 1 1 227.712 317.141333zm76.032 555.477334H151.68V384h152.064v488.618667zM948.266667.0h-872.704C33.792.0.0 33.024.0 73.770667v876.458666C0 991.018667 33.792 1024 75.562667 1024h872.576C989.866667 1024 1024 991.018667 1024 950.229333V73.770667C1024 33.024 989.866667.0 948.138667.0h.128z"/></svg></a><a href=http://github.com/owulveryck rel="me noopener" class=iconfont title=github target=_blank><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="36" height="36"><path d="M512 12.672c-282.88.0-512 229.248-512 512 0 226.261333 146.688 418.133333 350.08 485.76 25.6 4.821333 34.986667-11.008 34.986667-24.618667.0-12.16-.426667-44.373333-.64-87.04-142.421333 30.890667-172.458667-68.693333-172.458667-68.693333C188.672 770.986667 155.008 755.2 155.008 755.2c-46.378667-31.744 3.584-31.104 3.584-31.104 51.413333 3.584 78.421333 52.736 78.421333 52.736 45.653333 78.293333 119.850667 55.68 149.12 42.581333 4.608-33.109333 17.792-55.68 32.426667-68.48-113.706667-12.8-233.216-56.832-233.216-253.013333.0-55.893333 19.84-101.546667 52.693333-137.386667-5.76-12.928-23.04-64.981333 4.48-135.509333.0.0 42.88-13.738667 140.8 52.48 40.96-11.392 84.48-17.024 128-17.28 43.52.256 87.04 5.888 128 17.28 97.28-66.218667 140.16-52.48 140.16-52.48 27.52 70.528 10.24 122.581333 5.12 135.509333 32.64 35.84 52.48 81.493333 52.48 137.386667.0 196.693333-119.68 240-233.6 252.586667 17.92 15.36 34.56 46.762667 34.56 94.72.0 68.522667-.64 123.562667-.64 140.202666.0 13.44 8.96 29.44 35.2 24.32C877.44 942.592 1024 750.592 1024 524.672c0-282.752-229.248-512-512-512"/></svg></a><a href=https://owulveryck.github.io/index.xml rel="noopener alternate" type=application/rss&#43;xml class=iconfont title=rss target=_blank><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="30" height="30"><path d="M819.157333 1024C819.157333 574.592 449.408 204.8.0 204.8V0c561.706667.0 1024 462.293333 1024 1024H819.157333zM140.416 743.04a140.8 140.8.0 0 1 140.501333 140.586667A140.928 140.928.0 0 1 140.074667 1024C62.72 1024 0 961.109333.0 883.626667S62.933333 743.082667 140.416 743.04zM678.784 1024h-199.04c0-263.210667-216.533333-479.786667-479.744-479.786667V345.173333c372.352.0 678.784 306.517333 678.784 678.826667z"/></svg></a></div><div class=copyright><span class=power-by>Powered by <a class=hexo-link href=https://gohugo.io>Hugo</a></span>
<span class=division>|</span>
<span class=theme-info>Theme - <a class=theme-link href=https://github.com/xianmin/hugo-theme-jane>Jane</a></span>
<span class=copyright-year>&copy;
2015 -
2020
<span class=heart><i class=iconfont><svg class="icon" viewBox="0 0 1025 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="14" height="14"><path d="M1000.1 247.9c-15.5-37.3-37.6-70.6-65.7-98.9-54.4-54.8-125.8-85-201-85-85.7.0-166 39-221.4 107.4C456.6 103 376.3 64 290.6 64c-75.1.0-146.5 30.4-201.1 85.6-28.2 28.5-50.4 61.9-65.8 99.3-16 38.8-24 79.9-23.6 122.2.7 91.7 40.1 177.2 108.1 234.8 3.1 2.6 6 5.1 8.9 7.8 14.9 13.4 58 52.8 112.6 102.7 93.5 85.5 209.9 191.9 257.5 234.2 7 6.1 15.8 9.5 24.9 9.5 9.2.0 18.1-3.4 24.9-9.5 34.5-30.7 105.8-95.9 181.4-165 74.2-67.8 150.9-138 195.8-178.2 69.5-57.9 109.6-144.4 109.9-237.3.1-42.5-8-83.6-24-122.2z" fill="#8a8a8a"/></svg></i></span><span class=author>Olivier Wulveryck</span></span></div></footer><div class=back-to-top id=back-to-top><i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="35" height="35"><path d="M510.866688 227.694839 95.449397 629.218702h235.761562L329.15309 958.01517h362.40389L691.55698 628.188232l241.942331-3.089361L510.866688 227.694839zM63.840492 63.962777h894.052392v131.813095H63.840492V63.962777v0zm0 0"/></svg></i></div></div><script type=text/javascript src=/lib/jquery/jquery-3.2.1.min.js></script><script type=text/javascript src=/lib/slideout/slideout-1.0.1.min.js></script><script type=text/javascript src=/js/main.638251f4230630f0335d8c6748e53a96f94b72670920b60c09a56fdc8bece214.js integrity="sha256-Y4JR9CMGMPAzXYxnSOU6lvlLcmcJILYMCaVv3Ivs4hQ=" crossorigin=anonymous></script><script type=text/javascript src=/js/load-photoswipe.js></script><script type=text/javascript src=/lib/photoswipe/photoswipe.min.js></script><script type=text/javascript src=/lib/photoswipe/photoswipe-ui-default.min.js></script></body></html>