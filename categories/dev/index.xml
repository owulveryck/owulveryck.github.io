<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>dev on Unladen swallow - Olivier Wulveryck</title><link>https://owulveryck.github.io/categories/dev/</link><description>Recent content in dev on Unladen swallow - Olivier Wulveryck</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Olivier Wulveryck</copyright><lastBuildDate>Sat, 11 Mar 2017 09:15:17 +0100</lastBuildDate><atom:link href="https://owulveryck.github.io/categories/dev/index.xml" rel="self" type="application/rss+xml"/><item><title>350000 rows, 133 cols... From a huge CSV to DynamoDB (without breaking piggy-bank).</title><link>https://owulveryck.github.io/2017/03/11/350000-rows-133-cols...-from-a-huge-csv-to-dynamodb-without-breaking-piggy-bank./</link><pubDate>Sat, 11 Mar 2017 09:15:17 +0100</pubDate><guid>https://owulveryck.github.io/2017/03/11/350000-rows-133-cols...-from-a-huge-csv-to-dynamodb-without-breaking-piggy-bank./</guid><description>In this post I will explain how to:
Parse a CSV file and extract only certain columns Create a table in DynamoDB Insert all the data with an adaptive algorithm in order to use the provisioned capacity Reduce the capacity once the insertion is done. Exploring the problem: AWS Billing In a previous post I explained how I was using dynamodb to store a lot of data about aws billing.</description></item></channel></rss>