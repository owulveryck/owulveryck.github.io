---
title: Datamesh
date: 2021-03-01T13:11:25.330Z
draft: true
categories:
    - data-mesh
lastmod: 2021-03-01T13:11:37.246Z
---

Le data-mesh est un paradigme de gestion de la donnée. C'est à dire quie c'est un concept. Ce concept repose sur une modélisations des problèmes que posent la gestion de la donnée à grande échelle, mais également que ce système propose une solution à ces problèmes.

Le but de cet article est de présenter les fondements du datamesh et de poser quelques convictions issues de certaines expériences du terrain.

Dans un premier temps nous poserons un ensemble d'éléments structurants autour de la donnée. Ensuite nous poserons la définition du datamesh et des pilliers qui le compose. Enfin nous appliquerons ce principe sur un exemple d'architecture. La partie organisationnelle sera étudiée dans un prochain article.

{{< figure src="/assets/magritte.jpg" caption="Disclaimer: Cet article n'est pas un article sur les 'pipe-lines de données'" attr="" attrlink="" >}}

## La donnée, cette élément ubiquite des SI modernes.

Afin de clarifier la notion d'ubiquité, commençons par poser un peu de vocabulaire. Ainsi, nous définissons une donnée comme étant un ensemble de bits, et une information comme étant une donnée dont le sens est défini par un ensemble de règles.

Les conséquences de ces définitions sont que:

- la donnée n'est qu'un support d'information;
- l'information que l'on peut extraire d'une même donnée peut varier en fonction des règles de gestion.

La connaissance et l'ensemble de l'information structurée.

Dans le reste de ce document, nous utiliserons _donnée_ en lieu et place _d'information_ pour faciliter la lecture et la compréhension.

Ajoutons à présent deux définitions:

Nous qualifierons de *données opérationnelles*, les données nécessaires au fonctionnement du business. Ce sont l'ensemble des données qui permettent aux applications de réaliser des opérations métier. Ces données sont en général transactionnelles et sont souvent adressées en mode _Create, Read, Update, Delete_ (CRUD).

Si on considère la dimension temporelle des données opérationnelles et que l'on ajoute quelques règles de gestion métier, et nous obtenons des *données analytiques*. Les données analytiques sont porteuses de toutes les promesses d'optimisation du business (via la _Business Intelligence_ ou _BI_) et d'avantage concurentiel insufflés par l'air du big data et du machine learning.

L'ensemble des ces données (ou informations), constitue la connaissance business sous forme numérique.

### Gérer cette mine d'informations: le paradigme classique de la centralisation

Les données opérationnelles sont, de manière courrantes, gérées au plus près des applications. Ainsi, il est de plus en plus rare de voir de grosses bases de données centralisées qui regroupent l'ensemble des données opérationnelles en dehors de gros ERP.

Les données analytiques, quant-à elles, sont souvent regroupées dans des entrepots de données, voir dans des lacs de données (datalake). Le but de cette centralisation est de concentrer l'information pour rendre l'exploitation de la connaissance plus simple.

Néanmoins, ce système de centralisation pose d'autres problèmes. que nous allons tenter de décrire ici.

#### le manque de qualité et d'ownership des données

 En premier lieu la qualité des données posées dans les lacs n'est pas toujours au rendez-vous. En effet, nous constatons souvent que les données acheminées dans les lacs ne sont qu'un syphonage des données opérationnelles. 

#### la donnée en tant que conséquence

Les produits informatiques sont pensées autour des applications opérationnelles. Le coeur du produit est un domaine métier et la donnée ne reflette que rarement les éléments de ce domaine.

// TODO décrire la complexité des schémas / Data marecage

// TODO décrire le couplage

### Le datamesh, une gestion décentralisée de la donnée

Le datamesh propose d'adresser les problèmes évoqués précédemment en proposant un modèle de gestion de la données décentralisé. L'élément de base de ce système est un domaine de donnée. l'ensemble de ces domaines de données forme un maillage. La but de ce maillage reste le même que celui des lacs de données: proposer une vision d'ensemble de la connaissance exploitable dans le but d'augmenter la valeur business.

#### les pilliers du datamesh

Le but du paradigme est d'adresser le plus efficacement possible le passage à l'echelle de l'exploitation de la donnée selon plusieurs axes:

- la réponse au changement doit être rapide: les modèles de données évoluent constamment et de manière rapide;
- le nombre de producteurs de données est en croissance permanente (nouvelles applications, ouverture des API d'un SI sur l'extérieur);
- le nombre de consommateurs de donnée augmente (beaucoup d'initiatives de projets "data" voient le jour avec l'avènement du machine learning);

Le datamesh repose sur 4 grands pilliers:

- Découper la connaissance en un ensemble de noeuds élémentaires appelé "domaine de donnée" (Le terme domaine est emprunté au _domain driven design_ (DDD) d'Eric Evans).
- Placer la donnée au centre de ce domaine et penser la domaine comme un produit.
- Utiliser une infrastructure self-service en tant que support des data-products.
- Appliquer une gouvernance fédérée autour de la donnée.

## Le data-product et son domaine

Un domaine de donnée est un découpage logique de l'ensemble des données dont dispose un business dans lequel l'information a une signification et un cycle de vie déterminé.

Le data-domaine est découpé en fonction des domaines métiers qu'il sert.
La premier changement de paradigme se trouve dans le fait que les domaine de donnée vont fare coexister les données opérationnelles *et* les données analytiques.

Ainsi, la cission entre les procédés de gestions analytique et opérationnelle est supprimée. L'ensemble de l'informations gérées dans le domaine est gérée par un _data product owner_ dont le rôle sera de s'assurer que les capacités opérationnelles permettent d'obtenir de la donnée opérationnelle fiable, utile et propre dans le but d'obtenir et de fournir une information de qualité.
Le service opérationnel que fournit le datadomain est donc presque une conséquence et non une cause.

Nous pouvons séparer les clients des domaines en deux catégories:

- les producteurs de données qui apportent de la valeur au domaine en échange de fonctions opérationnelles (par exemple, toutes les applications d'un business ont accès au carnet d'adresse des clients)
- les consommateurs de données qui extraient de la valeur des applications pour augmenter la valeur du business (par exemple des équipes de data scientists)

Voici un exemple visuelle de data domaine en modélisation c4 (c4model.com).

{{< figure src="/assets/datamesh/C1.png" caption="C1 example of a datamesh architecture'" attr="" attrlink="" >}}

Au vu de cette architecture, nous pouvons faire les constats suivants:

- les pipelines de données sont absent de cette représentation car ils ne sont plus un système de gestion à part entière, mais un élément interne à chaque domaine.
- la décomposition des éléments n'est plus technologique (construit autour d'un système central de stockage objet), mais orientée autour de domaines pouvant être corrélés au business.

Le domaine a pour but de fédérer la signification des différents éléments tels que:

- la signification des données dans le contexte du domaine
- la signification des informations en regard du context métier.

_Remarque_ comme dans toute architecture distribuée (tels que les architectures à base de micro-services), un élément clé de réussite est d'assurer l'autonomie des noeuds et de maitriser le couplage entre les éléments. Par conséquent, il est souhaitable de disposer d'informations auto-suffisantes au sein d'un domaine. Ceci peut passer par une duplication ou une dénormalisation des informations entre les noeuds.

## La donnée en tant que produit

Le domaine de donnée a permis de délimiter les frontières dans lequelles la donnée a un sens, une valeur et une qualité précise.
Voyons à présent comment représenter la donnée en tant que produit dans ce contexte.

Traiter la donnée en tant que produit va permettre d'appliquer les principes du "product thinking" à notre contexte. Le but est de permettre aux consommateurs d'informations de pouvoir découvrir, comprendre et utiliser des données hautement qualitatives.

Pour Marty Cagan (cf [Product Is hard - video](https://www.youtube.com/watch?v=gCYFmrvPI8Q) ou [Inspired - livre](https://svpg.com/inspired-how-to-create-products-customers-love/)), un bon produit doit:

- (ap)porter de la valeur - les clients doivent choisir le produit pour la valeur qu'il apporte
- utilisable - son utilisation doit être evidente
- réalisable - en utilisant les compétences et la technologie a disposition

Ceci amène les le contrat technologiqe auxquelle devra répondre un data-products:

- être partagés et découvrables
- être auto-portants
- être utilisables
- être inter-opérables (gouverné par des standards d'échange)
- être sécurisés (gouvernés et contrôlés par une instances vérifiable et globale)
- proposer un contrat de confiance

> "Trust: a confident relationship to the unknown"- Rachel Botsman

Afin de permettre la mise en place de ces différents data-products, il est donc important de disposer d'outils de validation et d'échange globaux et standardisés.
Ainsi, par exemple, un système à base d'ontology permettra de définir les contours et interactions des différents data-products, les rendant auto-portant et découvrable dans l'écosystème.

Le but est de rentrer dans un cycle d'intelligence continue telle que décrite sur le site de [par Ken Collier, Mark Brand et Pramod N](http://thoughtworks.com/insights/articles/intelligent-enterprise-series-models-enterprise-intelligence)

![](https://dynamic.thoughtworks.com/landing_pages/inline_image_desktop-dde6dbc2c11220250eccb37cfb315115.png)

## Une gouvernance de donnée fédérée

## Un plateforme self-service

## Conclusion

{{< figure src="/assets/maillage-octo.png" caption="" attr="" attrlink="" >}}