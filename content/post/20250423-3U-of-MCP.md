---
title: "MCP's 3 U's: Making a Tool Useful, Usable, and Used by and for an LLM"
date: 2025-04-22T12:15:33+01:00
lastmod: 2025-04-22T12:15:33+01:00
images: [/assets/mcp/human-robot-tool.png]
draft: false
keywords: []
summary: 
tags: []
categories: []
author: "Olivier Wulveryck"
comment: false
toc: true
autoCollapseToc: false
# You can also define another contentCopyright.
contentCopyright: false
reward: false
mathjax: false
---

Since its announcement a couple of months ago, the Model Context Protocol (MCP) has gained significant attention.

Initially, MCP served as a straightforward mechanism, essentially a plugin system, to grant Large Language Models (LLMs) the ability to interact seamlessly with their environment, thus enhancing AI assistants and agents. Applying Wardley's theory to trace the protocol's evolution, I'd argue that MCP has progressed beyond Stage I (labeled _genesis_ in the image), establishing a *certainty of solution*. Furthermore, its widespread adoption suggests it has entered Phase II (labeled _Custom Built_ on the image).

![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjMFN3o1ujMDfd4y78hHCRFmPSTf9BP5C_Ej1jtEyZrmNC21aBw-18gAbVk88nKHdVa3gd_-D3z3pKKfO4Wa6XsIa1BuTkeiazqGLdu8vlUPsSaXeDgbkbvrMy3CSHlUiqk5ol1ig/s1600/Screen+Shot+2014-01-09+at+13.26.48.png)

_Note_ it is beyond the scope of this article to explain Wardley's theory. See the reference on [Simon Wardley's blog](https://blog.gardeviance.org/2014/03/on-mapping-and-evolution-axis.html) for more info)

Consequently, we're observing products *crafted with* MCP. The subsequent phase involves developing products *around* MCP.

Personally, my focus remains on exploring the protocol to identify the specific problems it can effectively address.

I previously blogged about a Proof of Concept (POC) developed during MCP's Stage I, a time when confirming the protocol's viability was paramount.

Currently, I interact with an AI that leverages various tools. My setup features middleware exposing a REST API compatible with OpenAI v1. As a user, I engage through a UI, which communicates with the middleware. The middleware, in turn, invokes the inference engine to provide input to the LLM and orchestrates calls to MCP tools in the order determined by the LLM.

Integrating MCP servers into the middleware is simple, instantly enabling them as capabilities accessible to the LLM.

The MCP protocol, along with its middleware implementation, ensures that any tool becomes *usable* by any LLM compatible with my inference engine (currently, I utilize Vertex AI from Google, with plans to incorporate support for Ollama in the future.)

This article focuses on understanding how to craft tools that fulfill the 3Us (**usable**, **useful**, **used**) from the perspective of the LLM. In this article, I will explore the notion of prompt exposition of the MCP protocol, showing why it is important to consider when moving from an MCP POC to an MCP product.

## `{JSON-RPC + NLP}`: _Organon Organōn_ (the tool of tools)

MCP provides an answer to the question: "_How does the user (the LLM in our case) interact with the tool?_" by offering a communication structure: JSON-RPC.

But, in fact, there are two languages involved in this communication, both stemming from the LLM trying to achieve its goal with a tool:

- JSON-RPC, as mentioned, governs tool manipulation.
- Natural Language supports *why* the user (again, the LLM) should use the tool.

Therefore, it is mandatory to have a clear description of the tool exposed through MCP, similar to proper documentation. LLMs understand our language, so we should provide them with our documentation.

Without this documentation—clear descriptions and illustrative prompts—the tool lacks **affordance** from the LLM's perspective. In human-computer interaction, affordance refers to the quality of an object or environment that allows an individual to perform an action. For an LLM interacting via text, affordance means the tool's textual description (its name, parameters, documentation, and associated prompts) must clearly signal *what it does* and *how to use it correctly*. If these textual cues are missing or ambiguous, the LLM cannot easily perceive the tool's utility or operational requirements, and consequently, the tool may not be **used**, even if it's potentially useful.

### Useful to ensure it is used


Simply documenting the tool is not the best way to provide affordance from the perspective of an LLM.

To further assist the LLM, the MCP server can expose prompts. When I first encountered prompt exposition in an MCP tool, I questioned its inclusion in the protocol.

Actions and Resources are easily understandable for programmers, like to POST and GET requests in a REST API.

But what about prompts?

According to the standard, MCP prompts enable servers to define reusable, user-controlled prompt templates and workflows.

My understanding is that prompt exposition helps ensure the tool is **useful** to the LLM.

Beside solving repetitive and complex problems, **they simplify the critical task of gathering and injecting relevant context (like specific code files or logs) needed for meaningful results, and they make the server's specific, high-value LLM capabilities easily discoverable and accessible to the end-user via the client interface.** This transforms abstract potential into concrete, readily available actions.

In short, MCP prompts helps the LLM (and the user interacting via the LLM or client) to answer those questions:

*   Why would someone use this tool: is this useful to solve my target problem?
*   But more important: **How do I use it effectively?** (By selecting the appropriate prompt and providing the required arguments, often simplified by the client UI).

![](/assets/mcp/human-robot-tool.png)

## Illustration through a use-case

Anthropic proposes a tool called [_memory_](https://github.com/modelcontextprotocol/servers/tree/main/src/memory). Memory is a knowledge graph-based system that can be used to share elements between contexts.
Think of it as a structureless database that can store relationships between different entities.

For example, you can store: "MCP's 3 U's: Making a Tool Useful, Usable, and Used by and for an LLM" "written by" "Olivier".

- "MCP's 3 U's: Making a Tool Useful, Usable, and Used by and for an LLM" and "Olivier" are nodes in the graph.
- "written by" is an edge.

The entity {"MCP's 3 U's: Making a Tool Useful, Usable, and Used by and for an LLM" "written by" "Olivier"} is a "triple," and one way to interact with the knowledge graph is to expose functions like "insert_triple" or "get_triple." The MCP service can then expose a set of resources to fetch elements based on their relationships (e.g., all elements "written by" "Olivier").

This is very powerful because it is schemaless: you can store anything in a node and any relationship between two nodes.

To represent knowledge, only a small set of functions is needed:

*   **Creation/Maintenance:** `add_triple` and `remove_triple` allow you to build and curate the graph.
*   **Retrieval:** `find_triples` is a versatile workhorse for retrieving specific facts or patterns.
*   **Exploration:** `describe_entity` provides the essential capability to navigate and understand the context around any given node, which is key to discovering insights and relationships in a graph structure.

We can then let the LLM analyze a text and create a structure accordingly. For example, here is what I asked Gemini (regarding the article you are reading):

> simulate the creation of a knowledge graph by writing all the calls to a function `add_triple(subject, predicate, object)` you would do to represent this article:
> 
> 
> Since its announcement a couple of months ago, the Model Context Protocol (MCP) has gained significant attention.
...

Its reply is:

```
# Core Concepts and Definitions
add_triple("Model Context Protocol", "alias", "MCP")
add_triple("MCP", "description", "A mechanism to grant LLMs the ability to interact seamlessly with their environment")
add_triple("MCP", "type", "Protocol")
add_triple("MCP", "initialPurpose", "Plugin system for LLMs")
add_triple("MCP", "enhances", "AI assistants")
add_triple("MCP", "enhances", "AI agents")

# Evolution and Stages (Wardley's Theory)
add_triple("MCP", "evolutionAnalyzedWith", "Wardley's theory")
add_triple("Stage I", "label", "genesis")
add_triple("MCP", "established", "certainty of solution")
add_triple("MCP", "enteredPhase", "Phase II")
add_triple("Phase II", "label", "Custom Built")
add_triple("MCP", "adoption", "widespread")
add_triple("Products", "areCraftedWith", "MCP") # Implied relationship in Phase II
add_triple("Products", "developedAround", "MCP") # Subsequent phase mentioned

# Author's Interaction and Setup
add_triple("Author", "focusesOn", "exploring MCP")
add_triple("Author", "focusesOn", "identifying problems MCP can address")
add_triple("Author", "bloggedAbout", "POC")
add_triple("POC", "type", "Proof of Concept")
add_triple("POC", "developedDuring", "Stage I")
add_triple("Author", "interactsWith", "AI")
...
```

So far, so good... and it worked. Using the MCP tool actually creates a file with this content that I can then query by asking:

> what does MCP enhance according to the graph?

```
MCP enhances AI assistants and AI agents
```

So the tool is Useful... but is it really usable? Take the following line:

`add_triple("MCP", "evolutionAnalyzedWith", "Wardley's theory")`

If I ask in a new session to update the memory with another article analysis, how can I be sure that it will use "evolutionAnalyzedWith" and not something else?

For example, I asked the same question to the same LLM in another context, and I received this answer:

`add_triple("Wardley's theory", "applied_to", "MCP evolution")`

This inconsistency is problematic. The LLM, trying to be helpful, invents relationship terms (`evolutionAnalyzedWith`, `applied_to`) that *seem* reasonable in isolation but lead to a fragmented and unreliable knowledge graph over time. Queries become difficult because you don't know which predicate term was used. The core issue is the lack of a predefined structure or vocabulary for the relationships the LLM is creating. The knowledge graph implementation, while flexible, is not *semantically* affordant – it doesn't guide the LLM on *how* to structure the knowledge consistently.

### Ontology, the missing part

An ontology is like a schema for the knowledge graph. In short, it exposes the properties that are allowed between nodes, as well as the type of each node. Using an ontology is one solution to this problem: it exposes which links can be used with which type of node.

But what does all of it have to do with MCP and prompts ?


